{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div style=\"text-align: center;\">\n",
        "    <h1> <font style=\"bold\"> Trabajo Práctico </font></h1>\n",
        "    <h2><font style=\"bold\">LDA/QDA y optimización matemática de modelos</font></h2>\n",
        "    <h3><font style=\"bold\">Abril Noguera - Pablo Brahim - Fermin Rodriguez - Kevin Pennington</font></h3>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4kL_4etdeizy"
      },
      "source": [
        "# Intro teórica\n",
        "\n",
        "## Definición: Clasificador Bayesiano\n",
        "\n",
        "Sean $k$ poblaciones, $x \\in \\mathbb{R}^p$ puede pertenecer a cualquiera $g \\in \\mathcal{G}$ de ellas. Bajo un esquema bayesiano, se define entonces $\\pi_j \\doteq P(G = j)$ la probabilidad *a priori* de que $X$ pertenezca a la clase *j*, y se **asume conocida** la distribución condicional de cada observable dado su clase $f_j \\doteq f_{X|G=j}$.\n",
        "\n",
        "De esta manera dicha probabilidad *a posteriori* resulta\n",
        "$$\n",
        "P(G|_{X=x} = j) = \\frac{f_{X|G=j}(x) \\cdot p_G(j)}{f_X(x)} \\propto f_j(x) \\cdot \\pi_j\n",
        "$$\n",
        "\n",
        "La regla de decisión de Bayes es entonces\n",
        "$$\n",
        "H(x) \\doteq \\arg \\max_{g \\in \\mathcal{G}} \\{ P(G|_{X=x} = j) \\} = \\arg \\max_{g \\in \\mathcal{G}} \\{ f_j(x) \\cdot \\pi_j \\}\n",
        "$$\n",
        "\n",
        "es decir, se predice a $x$ como perteneciente a la población $j$ cuya probabilidad a posteriori es máxima.\n",
        "\n",
        "*Ojo, a no desesperar! $\\pi_j$ no es otra cosa que una constante prefijada, y $f_j$ es, en su esencia, un campo escalar de $x$ a simplemente evaluar.*\n",
        "\n",
        "## Distribución condicional\n",
        "\n",
        "Para los clasificadores de discriminante cuadrático y lineal (QDA/LDA) se asume que $X|_{G=j} \\sim \\mathcal{N}_p(\\mu_j, \\Sigma_j)$, es decir, se asume que cada población sigue una distribución normal.\n",
        "\n",
        "Por definición, se tiene entonces que para una clase $j$:\n",
        "$$\n",
        "f_j(x) = \\frac{1}{(2 \\pi)^\\frac{p}{2} \\cdot |\\Sigma_j|^\\frac{1}{2}} e^{- \\frac{1}{2}(x-\\mu_j)^T \\Sigma_j^{-1} (x- \\mu_j)}\n",
        "$$\n",
        "\n",
        "Aplicando logaritmo (que al ser una función estrictamente creciente no afecta el cálculo de máximos/mínimos), queda algo mucho más práctico de trabajar:\n",
        "\n",
        "$$\n",
        "\\log{f_j(x)} = -\\frac{1}{2}\\log |\\Sigma_j| - \\frac{1}{2} (x-\\mu_j)^T \\Sigma_j^{-1} (x- \\mu_j) + C\n",
        "$$\n",
        "\n",
        "Observar que en este caso $C=-\\frac{p}{2} \\log(2\\pi)$, pero no se tiene en cuenta ya que al tener una constante aditiva en todas las clases, no afecta al cálculo del máximo.\n",
        "\n",
        "## LDA\n",
        "\n",
        "En el caso de LDA se hace una suposición extra, que es $X|_{G=j} \\sim \\mathcal{N}_p(\\mu_j, \\Sigma)$, es decir que las poblaciones no sólo siguen una distribución normal sino que son de igual matriz de covarianzas. Reemplazando arriba se obtiene entonces:\n",
        "\n",
        "$$\n",
        "\\log{f_j(x)} =  -\\frac{1}{2}\\log |\\Sigma| - \\frac{1}{2} (x-\\mu_j)^T \\Sigma^{-1} (x- \\mu_j) + C\n",
        "$$\n",
        "\n",
        "Ahora, como $-\\frac{1}{2}\\log |\\Sigma|$ es común a todas las clases se puede incorporar a la constante aditiva y, distribuyendo y reagrupando términos sobre $(x-\\mu_j)^T \\Sigma^{-1} (x- \\mu_j)$ se obtiene finalmente:\n",
        "\n",
        "$$\n",
        "\\log{f_j(x)} =  \\mu_j^T \\Sigma^{-1} (x- \\frac{1}{2} \\mu_j) + C'\n",
        "$$\n",
        "\n",
        "## Entrenamiento/Ajuste\n",
        "\n",
        "Obsérvese que para ambos modelos, ajustarlos a los datos implica estimar los parámetros $(\\mu_j, \\Sigma_j) \\; \\forall j = 1, \\dots, k$ en el caso de QDA, y $(\\mu_j, \\Sigma)$ para LDA.\n",
        "\n",
        "Estos parámetros se estiman por máxima verosimilitud, de manera que los estimadores resultan:\n",
        "\n",
        "* $\\hat{\\mu}_j = \\bar{x}_j$ el promedio de los $x$ de la clase *j*\n",
        "* $\\hat{\\Sigma}_j = s^2_j$ la matriz de covarianzas estimada para cada clase *j*\n",
        "* $\\hat{\\pi}_j = f_{R_j} = \\frac{n_j}{n}$ la frecuencia relativa de la clase *j* en la muestra\n",
        "* $\\hat{\\Sigma} = \\frac{1}{n} \\sum_{j=1}^k n_j \\cdot s^2_j$ el promedio ponderado (por frecs. relativas) de las matrices de covarianzas de todas las clases. *Observar que se utiliza el estimador de MV y no el insesgado*\n",
        "\n",
        "Es importante notar que si bien todos los $\\mu, \\Sigma$ deben ser estimados, la distribución *a priori* puede no inferirse de los datos sino asumirse previamente, utilizándose como entrada del modelo.\n",
        "\n",
        "## Predicción\n",
        "\n",
        "Para estos modelos, al igual que para cualquier clasificador Bayesiano del tipo antes visto, la estimación de la clase es por método *plug-in* sobre la regla de decisión $H(x)$, es decir devolver la clase que maximiza $\\hat{f}_j(x) \\cdot \\hat{\\pi}_j$, o lo que es lo mismo $\\log\\hat{f}_j(x) + \\log\\hat{\\pi}_j$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IV8OF-SlPHbD"
      },
      "source": [
        "# Código provisto\n",
        "\n",
        "Con el fin de no retrasar al alumno con cuestiones estructurales y/o secundarias al tema que se pretende tratar, se provee una base de código que **no es obligatoria de usar** pero se asume que resulta resulta beneficiosa."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 185,
      "metadata": {
        "id": "PrDdJRypNB-y"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import numpy.linalg as LA\n",
        "from scipy.linalg import cholesky, solve_triangular\n",
        "from scipy.linalg.lapack import dtrtri"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5cPL33WIN2HA"
      },
      "source": [
        "## Base code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 186,
      "metadata": {
        "id": "ewg5e0hsNTQC"
      },
      "outputs": [],
      "source": [
        "class BaseBayesianClassifier:\n",
        "  def __init__(self):\n",
        "    pass\n",
        "\n",
        "  def _estimate_a_priori(self, y):\n",
        "    a_priori = np.bincount(y.flatten().astype(int)) / y.size\n",
        "    # Q3: para que sirve bincount? --> cuenta cuantas veces aparece cada clase en y.\n",
        "    return np.log(a_priori)\n",
        "\n",
        "  def _fit_params(self, X, y):\n",
        "    # estimate all needed parameters for given model\n",
        "    raise NotImplementedError()\n",
        "\n",
        "  def _predict_log_conditional(self, x, class_idx):\n",
        "    # predict the log(P(x|G=class_idx)), the log of the conditional probability of x given the class\n",
        "    # this should depend on the model used\n",
        "    raise NotImplementedError()\n",
        "\n",
        "  def fit(self, X, y, a_priori=None):\n",
        "    # if it's needed, estimate a priori probabilities\n",
        "    self.log_a_priori = self._estimate_a_priori(y) if a_priori is None else np.log(a_priori)\n",
        "\n",
        "    # now that everything else is in place, estimate all needed parameters for given model\n",
        "    self._fit_params(X, y)\n",
        "    # Q4: por que el _fit_params va al final? no se puede mover a, por ejemplo, antes de la priori? --> \n",
        "\n",
        "  def predict(self, X):\n",
        "    # this is actually an individual prediction encased in a for-loop\n",
        "    m_obs = X.shape[1]\n",
        "    y_hat = np.empty(m_obs, dtype=int)\n",
        "\n",
        "    for i in range(m_obs):\n",
        "      y_hat[i] = self._predict_one(X[:,i].reshape(-1,1))\n",
        "\n",
        "    # return prediction as a row vector (matching y)\n",
        "    return y_hat.reshape(1,-1)\n",
        "\n",
        "  def _predict_one(self, x):\n",
        "    # calculate all log posteriori probabilities (actually, +C)\n",
        "    log_posteriori = [ log_a_priori_i + self._predict_log_conditional(x, idx) for idx, log_a_priori_i\n",
        "                  in enumerate(self.log_a_priori) ]\n",
        "\n",
        "    # return the class that has maximum a posteriori probability\n",
        "    return np.argmax(log_posteriori)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 187,
      "metadata": {
        "id": "Rz2FC7A5NUpN"
      },
      "outputs": [],
      "source": [
        "class QDA(BaseBayesianClassifier):\n",
        "\n",
        "  def _fit_params(self, X, y):\n",
        "    # estimate each covariance matrix\n",
        "    self.inv_covs = [LA.inv(np.cov(X[:,y.flatten()==idx], bias=True))\n",
        "                      for idx in range(len(self.log_a_priori))]\n",
        "    # Q5: por que hace falta el flatten y no se puede directamente X[:,y==idx]?\n",
        "    # Q6: por que se usa bias=True en vez del default bias=False?\n",
        "    self.means = [X[:,y.flatten()==idx].mean(axis=1, keepdims=True)\n",
        "                  for idx in range(len(self.log_a_priori))]\n",
        "    # Q7: que hace axis=1? por que no axis=0?\n",
        "\n",
        "  def _predict_log_conditional(self, x, class_idx):\n",
        "    # predict the log(P(x|G=class_idx)), the log of the conditional probability of x given the class\n",
        "    # this should depend on the model used\n",
        "    inv_cov = self.inv_covs[class_idx]\n",
        "    unbiased_x =  x - self.means[class_idx]\n",
        "    return 0.5*np.log(LA.det(inv_cov)) -0.5 * unbiased_x.T @ inv_cov @ unbiased_x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 188,
      "metadata": {
        "id": "9lZbID0WNV1Y"
      },
      "outputs": [],
      "source": [
        "class TensorizedQDA(QDA):\n",
        "\n",
        "    def _fit_params(self, X, y):\n",
        "        # ask plain QDA to fit params\n",
        "        super()._fit_params(X,y)\n",
        "\n",
        "        # stack onto new dimension\n",
        "        self.tensor_inv_cov = np.stack(self.inv_covs) # → shape: (k, p, p)\n",
        "        self.tensor_means = np.stack(self.means) # → shape: (k, p, 1)\n",
        "\n",
        "    def _predict_log_conditionals(self,x):\n",
        "        unbiased_x = x - self.tensor_means # → shape: (k, p, 1)\n",
        "        inner_prod = unbiased_x.transpose(0,2,1) @ self.tensor_inv_cov @ unbiased_x # → shape: (k, 1, 1)\n",
        "\n",
        "        return 0.5*np.log(LA.det(self.tensor_inv_cov)) - 0.5 * inner_prod.flatten() # → shape: (k, )\n",
        "\n",
        "    def _predict_one(self, x):\n",
        "        # return the class that has maximum a posteriori probability\n",
        "        return np.argmax(self.log_a_priori + self._predict_log_conditionals(x))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 189,
      "metadata": {
        "id": "i-WGGi_sQ-pT"
      },
      "outputs": [],
      "source": [
        "class QDA_Chol1(BaseBayesianClassifier):\n",
        "  def _fit_params(self, X, y):\n",
        "    self.L_invs = [\n",
        "        inv(cholesky(np.cov(X[:,y.flatten()==idx], bias=True), lower=True))\n",
        "        for idx in range(len(self.log_a_priori))\n",
        "    ]\n",
        "\n",
        "    self.means = [X[:,y.flatten()==idx].mean(axis=1, keepdims=True)\n",
        "                  for idx in range(len(self.log_a_priori))]\n",
        "\n",
        "  def _predict_log_conditional(self, x, class_idx):\n",
        "    L_inv = self.L_invs[class_idx]\n",
        "    unbiased_x =  x - self.means[class_idx]\n",
        "\n",
        "    y = L_inv @ unbiased_x\n",
        "\n",
        "    return np.log(L_inv.diagonal().prod()) -0.5 * (y**2).sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 190,
      "metadata": {
        "id": "i5DNLtYbQsHi"
      },
      "outputs": [],
      "source": [
        "class QDA_Chol2(BaseBayesianClassifier):\n",
        "  def _fit_params(self, X, y):\n",
        "    self.Ls = [\n",
        "        cholesky(np.cov(X[:,y.flatten()==idx], bias=True), lower=True)\n",
        "        for idx in range(len(self.log_a_priori))\n",
        "    ]\n",
        "\n",
        "    self.means = [X[:,y.flatten()==idx].mean(axis=1, keepdims=True)\n",
        "                  for idx in range(len(self.log_a_priori))]\n",
        "\n",
        "  def _predict_log_conditional(self, x, class_idx):\n",
        "    L = self.Ls[class_idx]\n",
        "    unbiased_x =  x - self.means[class_idx]\n",
        "\n",
        "    y = solve_triangular(L, unbiased_x, lower=True)\n",
        "\n",
        "    return -np.log(L.diagonal().prod()) -0.5 * (y**2).sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 191,
      "metadata": {
        "id": "v0dRvYVQRCgc"
      },
      "outputs": [],
      "source": [
        "class QDA_Chol3(BaseBayesianClassifier):\n",
        "  def _fit_params(self, X, y):\n",
        "    self.L_invs = [\n",
        "        dtrtri(cholesky(np.cov(X[:,y.flatten()==idx], bias=True), lower=True), lower=1)[0]\n",
        "        for idx in range(len(self.log_a_priori))\n",
        "    ]\n",
        "\n",
        "    self.means = [X[:,y.flatten()==idx].mean(axis=1, keepdims=True)\n",
        "                  for idx in range(len(self.log_a_priori))]\n",
        "\n",
        "  def _predict_log_conditional(self, x, class_idx):\n",
        "    L_inv = self.L_invs[class_idx]\n",
        "    unbiased_x =  x - self.means[class_idx]\n",
        "\n",
        "    y = L_inv @ unbiased_x\n",
        "\n",
        "    return np.log(L_inv.diagonal().prod()) -0.5 * (y**2).sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JCtrHQDuN6R4"
      },
      "source": [
        "## Datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 192,
      "metadata": {
        "id": "rasInBMFNzUH"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import load_iris, fetch_openml, load_wine\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "def get_iris_dataset():\n",
        "  data = load_iris()\n",
        "  X_full = data.data\n",
        "  y_full = np.array([data.target_names[y] for y in data.target.reshape(-1,1)])\n",
        "  return X_full, y_full\n",
        "\n",
        "def get_penguins_dataset():\n",
        "    # get data\n",
        "    df, tgt = fetch_openml(name=\"penguins\", return_X_y=True, as_frame=True, parser='auto')\n",
        "\n",
        "    # drop non-numeric columns\n",
        "    df.drop(columns=[\"island\",\"sex\"], inplace=True)\n",
        "\n",
        "    # drop rows with missing values\n",
        "    mask = df.isna().sum(axis=1) == 0\n",
        "    df = df[mask]\n",
        "    tgt = tgt[mask]\n",
        "\n",
        "    return df.values, tgt.to_numpy().reshape(-1,1)\n",
        "\n",
        "def get_wine_dataset():\n",
        "    # get data\n",
        "    data = load_wine()\n",
        "    X_full = data.data\n",
        "    y_full = np.array([data.target_names[y] for y in data.target.reshape(-1,1)])\n",
        "    return X_full, y_full\n",
        "\n",
        "def get_letters_dataset():\n",
        "    # get data\n",
        "    letter = fetch_openml('letter', version=1, as_frame=False)\n",
        "    return letter.data, letter.target.reshape(-1,1)\n",
        "\n",
        "def label_encode(y_full):\n",
        "    return LabelEncoder().fit_transform(y_full.flatten()).reshape(y_full.shape)\n",
        "\n",
        "def split_transpose(X, y, test_size, random_state):\n",
        "    # X_train, X_test, y_train, y_test but all transposed\n",
        "    return [elem.T for elem in train_test_split(X, y, test_size=test_size, random_state=random_state)]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ybPkuBdDN42P"
      },
      "source": [
        "## Benchmarking\n",
        "\n",
        "Nota: esta clase fue creada bastante rápido y no pretende ser una plataforma súper confiable sobre la que basarse, sino más bien una herramienta simple con la que poder medir varios runs y agregar la información.\n",
        "\n",
        "En forma rápida, `warmup` es la cantidad de runs para warmup, `mem_runs` es la cantidad de runs en las que se mide el pico de uso de RAM y `n_runs` es la cantidad de runs en las que se miden tiempos.\n",
        "\n",
        "La razón por la que se separan es que medir memoria hace ~2.5x más lento cada run, pero al mismo tiempo se estabiliza mucho más rápido.\n",
        "\n",
        "**Importante:** tener en cuenta que los modelos que predicen en batch (usan `predict` directamente) deberían consumir, como mínimo, $n$ veces la memoria de los que predicen por observación."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 193,
      "metadata": {
        "id": "nO4Py3CeNpKu"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "from tqdm import tqdm\n",
        "from numpy.random import RandomState\n",
        "import tracemalloc\n",
        "\n",
        "RNG_SEED = 6553\n",
        "\n",
        "class Benchmark:\n",
        "    def __init__(self, X, y, n_runs=1000, warmup=100, mem_runs=100, test_sz=0.3, rng_seed=RNG_SEED, same_splits=True):\n",
        "        self.X = X\n",
        "        self.y = y\n",
        "        self.n = n_runs\n",
        "        self.warmup = warmup\n",
        "        self.mem_runs = mem_runs\n",
        "        self.test_sz = test_sz\n",
        "        self.det = same_splits\n",
        "        if self.det:\n",
        "            self.rng_seed = rng_seed\n",
        "        else:\n",
        "            self.rng = RandomState(rng_seed)\n",
        "\n",
        "        self.data = dict()\n",
        "\n",
        "        print(\"Benching params:\")\n",
        "        print(\"Total runs:\",self.warmup+self.mem_runs+self.n)\n",
        "        print(\"Warmup runs:\",self.warmup)\n",
        "        print(\"Peak Memory usage runs:\", self.mem_runs)\n",
        "        print(\"Running time runs:\", self.n)\n",
        "        approx_test_sz = int(self.y.size * self.test_sz)\n",
        "        print(\"Train size rows (approx):\",self.y.size - approx_test_sz)\n",
        "        print(\"Test size rows (approx):\",approx_test_sz)\n",
        "        print(\"Test size fraction:\",self.test_sz)\n",
        "\n",
        "    def bench(self, model_class, **kwargs):\n",
        "        name = model_class.__name__\n",
        "        time_data = np.empty((self.n, 3), dtype=float)  # train_time, test_time, accuracy\n",
        "        mem_data = np.empty((self.mem_runs, 2), dtype=float)  # train_peak_mem, test_peak_mem\n",
        "        rng = RandomState(self.rng_seed) if self.det else self.rng\n",
        "\n",
        "\n",
        "        for i in range(self.warmup):\n",
        "            # Instantiate model with error check for unsupported parameters\n",
        "            model = model_class(**kwargs)\n",
        "\n",
        "            # Generate current train-test split\n",
        "            X_train, X_test, y_train, y_test = split_transpose(\n",
        "                self.X, self.y,\n",
        "                test_size=self.test_sz,\n",
        "                random_state=rng\n",
        "            )\n",
        "            # Run training and prediction (timing or memory measurement not recorded)\n",
        "            model.fit(X_train, y_train)\n",
        "            model.predict(X_test)\n",
        "\n",
        "        for i in tqdm(range(self.mem_runs), total=self.mem_runs, desc=f\"{name} (MEM)\"):\n",
        "\n",
        "            model = model_class(**kwargs)\n",
        "\n",
        "            X_train, X_test, y_train, y_test = split_transpose(\n",
        "                self.X, self.y,\n",
        "                test_size=self.test_sz,\n",
        "                random_state=rng\n",
        "            )\n",
        "\n",
        "            tracemalloc.start()\n",
        "\n",
        "            t1 = time.perf_counter()\n",
        "            model.fit(X_train, y_train)\n",
        "            t2 = time.perf_counter()\n",
        "\n",
        "            _, train_peak = tracemalloc.get_traced_memory()\n",
        "            tracemalloc.reset_peak()\n",
        "\n",
        "            model.predict(X_test)\n",
        "            t3 = time.perf_counter()\n",
        "            _, test_peak = tracemalloc.get_traced_memory()\n",
        "            tracemalloc.stop()\n",
        "\n",
        "            mem_data[i,] = (\n",
        "                train_peak / (1024 * 1024),\n",
        "                test_peak / (1024 * 1024)\n",
        "            )\n",
        "\n",
        "        for i in tqdm(range(self.n), total=self.n, desc=f\"{name} (TIME)\"):\n",
        "            model = model_class(**kwargs)\n",
        "\n",
        "            X_train, X_test, y_train, y_test = split_transpose(\n",
        "                self.X, self.y,\n",
        "                test_size=self.test_sz,\n",
        "                random_state=rng\n",
        "            )\n",
        "\n",
        "            t1 = time.perf_counter()\n",
        "            model.fit(X_train, y_train)\n",
        "            t2 = time.perf_counter()\n",
        "            preds = model.predict(X_test)\n",
        "            t3 = time.perf_counter()\n",
        "\n",
        "            time_data[i,] = (\n",
        "                (t2 - t1) * 1000,\n",
        "                (t3 - t2) * 1000,\n",
        "                (y_test.flatten() == preds.flatten()).mean()\n",
        "            )\n",
        "\n",
        "        self.data[name] = (time_data, mem_data)\n",
        "\n",
        "    def summary(self, baseline=None):\n",
        "        aux = []\n",
        "        for name, (time_data, mem_data) in self.data.items():\n",
        "            result = {\n",
        "                'model': name,\n",
        "                'train_mean_ms': time_data[:, 0].mean(),\n",
        "                'train_std_ms': time_data[:, 0].std(),\n",
        "                'test_mean_ms': time_data[:, 1].mean(),\n",
        "                'test_std_ms': time_data[:, 1].std(),\n",
        "                'mean_accuracy': time_data[:, 2].mean(),\n",
        "                'train_mem_mean_mb': mem_data[:, 0].mean(),\n",
        "                'train_mem_std_mb': mem_data[:, 0].std(),\n",
        "                'test_mem_mean_mb': mem_data[:, 1].mean(),\n",
        "                'test_mem_std_mb': mem_data[:, 1].std()\n",
        "            }\n",
        "            aux.append(result)\n",
        "        df = pd.DataFrame(aux).set_index('model')\n",
        "\n",
        "        if baseline is not None and baseline in self.data:\n",
        "            df['train_speedup'] = df.loc[baseline, 'train_mean_ms'] / df['train_mean_ms']\n",
        "            df['test_speedup'] = df.loc[baseline, 'test_mean_ms'] / df['test_mean_ms']\n",
        "            df['train_mem_reduction'] = df.loc[baseline, 'train_mem_mean_mb'] / df['train_mem_mean_mb']\n",
        "            df['test_mem_reduction'] = df.loc[baseline, 'test_mem_mean_mb'] / df['test_mem_mean_mb']\n",
        "        return df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mb5VEpEugFXW"
      },
      "source": [
        "## Ejemplo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 194,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fLyr4-hdgJ7e",
        "outputId": "bfa9623f-2baf-4735-96b5-c714b244b8da"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "((178, 13), (178, 1))"
            ]
          },
          "execution_count": 194,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# levantamos el dataset Wine, que tiene 13 features y 178 observaciones en total\n",
        "X_full, y_full = get_wine_dataset()\n",
        "\n",
        "X_full.shape, y_full.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 195,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZxQlFUSbgYHQ",
        "outputId": "dd396038-8bab-4ebd-b981-752526d8c98c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(array([['class_0'],\n",
              "        ['class_0'],\n",
              "        ['class_0'],\n",
              "        ['class_0'],\n",
              "        ['class_0']], dtype='<U7'),\n",
              " array([[0],\n",
              "        [0],\n",
              "        [0],\n",
              "        [0],\n",
              "        [0]]))"
            ]
          },
          "execution_count": 195,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# encodeamos a número las clases\n",
        "y_full_encoded = label_encode(y_full)\n",
        "\n",
        "y_full[:5], y_full_encoded[:5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 196,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pSBNNUOmgtsI",
        "outputId": "d29b11de-b5a9-4fa3-f009-d8780104fa64"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Benching params:\n",
            "Total runs: 140\n",
            "Warmup runs: 20\n",
            "Peak Memory usage runs: 20\n",
            "Running time runs: 100\n",
            "Train size rows (approx): 125\n",
            "Test size rows (approx): 53\n",
            "Test size fraction: 0.3\n"
          ]
        }
      ],
      "source": [
        "# generamos el benchmark\n",
        "# observar que son valores muy bajos de runs para que corra rápido ahora\n",
        "b = Benchmark(\n",
        "    X_full, y_full_encoded,\n",
        "    n_runs = 100,\n",
        "    warmup = 20,\n",
        "    mem_runs = 20,\n",
        "    test_sz = 0.3,\n",
        "    same_splits = False\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 197,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81,
          "referenced_widgets": [
            "2add89be4e944f4fbd91c5f1d459b5cb",
            "e0a05e28520841bd95e2aec19da024bd",
            "c406ae72dbf14856971c0ac3ad078062",
            "ca156cc465e1415b8df2a570abfa3ffe",
            "40416335cca64bbbad37afc46049363d",
            "4531ea3385d34454865e3c6ced188122",
            "a9bc2a66f8a84516a44d8d3ad8e885e2",
            "43ef5277fff74410bb85f09e64c37cfe",
            "d7276daf0b654112aab411ae3f14649f",
            "68a99e1ecb9741f688cb33bbfc19d5d2",
            "cec5bcb756694268abfc4eefad72f758",
            "0878ca0785b74f4fa8abce52c184ce33",
            "9cb5b6651bac47d0b99c9388def7d2e6",
            "ec4d02ef09c5492ab097ef02b9d3e2fb",
            "ed4b7065efa24e5da4f69c047647c047",
            "08dc6e2b043f409f8f8df2b63f6bc152",
            "f7bc1a202cde4351a6f98ae19393cf94",
            "caea67fec94c45a097cae720b582c68d",
            "c136c5327b804c74b0582ed04c8825dc",
            "b2d747a3bfdd4f4797f7e4b01964aa7b",
            "a5957e8514974187838fc2d43d548433",
            "38dcdaf521fe4043b7bc5fac762867e1"
          ]
        },
        "id": "zUciOjazhUu5",
        "outputId": "91fc0889-7a87-418e-9950-88371c912be9"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "QDA (MEM): 100%|██████████| 20/20 [00:00<00:00, 148.88it/s]\n",
            "QDA (TIME): 100%|██████████| 100/100 [00:00<00:00, 511.54it/s]\n"
          ]
        }
      ],
      "source": [
        "# bencheamos un par\n",
        "to_bench = [QDA]\n",
        "\n",
        "for model in to_bench:\n",
        "    b.bench(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 198,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81,
          "referenced_widgets": [
            "e3704aaa731c464a8d6c63c39d0b167a",
            "ee002c5199874f8db461bb1631f198ac",
            "9e38484f037e4f86be6b768459404767",
            "d670b24d966f4de1872524684ea98840",
            "74859eadc1ef4090a07e851f5949f4d6",
            "1f6553e18f4f460ebbc4b9705cfb9c3d",
            "44cc54751afe4e618d81e92e2e64ca58",
            "f1cdc4724c6d441cbebca60f1bb11aea",
            "4965b54e63794e0b80c19c150b072eda",
            "4c09e94a9e494e6daa6f59c23b8bcb4f",
            "ab3b1739c3bf4f3ba399e3a60d043ec8",
            "6bbe1b2d6f3846168244aca7ecf761f0",
            "eb6e454a891d493b88fe82442a9288e2",
            "3fad52f5ac204004afac2df2b55cbc7b",
            "2d6622ab061342f280f7fbcbc920bc00",
            "e7cdc4320bff45f6acf901b8bd9bdf1a",
            "bf6f0511eeca467c870c6a261712ca7e",
            "26d56661a9e4490c96ef5dd831fa904b",
            "ba742120ade4495d9f75b066c43b6ffb",
            "e38ba66c86764d578b97aa127e0cd9ce",
            "f87908723b244cd99144a4e1b7be8807",
            "568fd1f1b5984675bf1716520a63b7d1"
          ]
        },
        "id": "wpPhSSCNhlvG",
        "outputId": "a5302dc3-d947-47db-d96d-cf7a98152b0e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "TensorizedQDA (MEM): 100%|██████████| 20/20 [00:00<00:00, 337.32it/s]\n",
            "TensorizedQDA (TIME): 100%|██████████| 100/100 [00:00<00:00, 902.93it/s]\n"
          ]
        }
      ],
      "source": [
        "# como es una clase, podemos seguir bencheando más después\n",
        "b.bench(TensorizedQDA)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 199,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 163
        },
        "id": "bZ5-vowshr5c",
        "outputId": "f17bc091-0cf5-42b9-cd9a-1e3d61e824b0"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>train_mean_ms</th>\n",
              "      <th>train_std_ms</th>\n",
              "      <th>test_mean_ms</th>\n",
              "      <th>test_std_ms</th>\n",
              "      <th>mean_accuracy</th>\n",
              "      <th>train_mem_mean_mb</th>\n",
              "      <th>train_mem_std_mb</th>\n",
              "      <th>test_mem_mean_mb</th>\n",
              "      <th>test_mem_std_mb</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>model</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>QDA</th>\n",
              "      <td>0.177432</td>\n",
              "      <td>0.065318</td>\n",
              "      <td>1.609726</td>\n",
              "      <td>0.099622</td>\n",
              "      <td>0.982407</td>\n",
              "      <td>0.018669</td>\n",
              "      <td>0.000645</td>\n",
              "      <td>0.008124</td>\n",
              "      <td>0.000311</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>TensorizedQDA</th>\n",
              "      <td>0.173203</td>\n",
              "      <td>0.051770</td>\n",
              "      <td>0.777053</td>\n",
              "      <td>0.081978</td>\n",
              "      <td>0.982593</td>\n",
              "      <td>0.018443</td>\n",
              "      <td>0.000693</td>\n",
              "      <td>0.012103</td>\n",
              "      <td>0.000227</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "               train_mean_ms  train_std_ms  test_mean_ms  test_std_ms  \\\n",
              "model                                                                   \n",
              "QDA                 0.177432      0.065318      1.609726     0.099622   \n",
              "TensorizedQDA       0.173203      0.051770      0.777053     0.081978   \n",
              "\n",
              "               mean_accuracy  train_mem_mean_mb  train_mem_std_mb  \\\n",
              "model                                                               \n",
              "QDA                 0.982407           0.018669          0.000645   \n",
              "TensorizedQDA       0.982593           0.018443          0.000693   \n",
              "\n",
              "               test_mem_mean_mb  test_mem_std_mb  \n",
              "model                                             \n",
              "QDA                    0.008124         0.000311  \n",
              "TensorizedQDA          0.012103         0.000227  "
            ]
          },
          "execution_count": 199,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# hacemos un summary\n",
        "b.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 200,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        },
        "id": "09eKXqlXhwL-",
        "outputId": "d42734a6-6fd8-4b15-da84-144d4923113f"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>train_mean_ms</th>\n",
              "      <th>test_mean_ms</th>\n",
              "      <th>mean_accuracy</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>model</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>QDA</th>\n",
              "      <td>0.177432</td>\n",
              "      <td>1.609726</td>\n",
              "      <td>0.982407</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>TensorizedQDA</th>\n",
              "      <td>0.173203</td>\n",
              "      <td>0.777053</td>\n",
              "      <td>0.982593</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "               train_mean_ms  test_mean_ms  mean_accuracy\n",
              "model                                                    \n",
              "QDA                 0.177432      1.609726       0.982407\n",
              "TensorizedQDA       0.173203      0.777053       0.982593"
            ]
          },
          "execution_count": 200,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# son muchos datos! nos quedamos con un par nomás\n",
        "summ = b.summary()\n",
        "\n",
        "# como es un pandas DataFrame, subseteamos columnas fácil\n",
        "summ[['train_mean_ms', 'test_mean_ms','mean_accuracy']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 201,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 163
        },
        "id": "EopB9574h8I5",
        "outputId": "c2bd86ab-3ba3-456d-c99b-d3bd131af75f"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>train_mean_ms</th>\n",
              "      <th>train_std_ms</th>\n",
              "      <th>test_mean_ms</th>\n",
              "      <th>test_std_ms</th>\n",
              "      <th>mean_accuracy</th>\n",
              "      <th>train_mem_mean_mb</th>\n",
              "      <th>train_mem_std_mb</th>\n",
              "      <th>test_mem_mean_mb</th>\n",
              "      <th>test_mem_std_mb</th>\n",
              "      <th>train_speedup</th>\n",
              "      <th>test_speedup</th>\n",
              "      <th>train_mem_reduction</th>\n",
              "      <th>test_mem_reduction</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>model</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>QDA</th>\n",
              "      <td>0.177432</td>\n",
              "      <td>0.065318</td>\n",
              "      <td>1.609726</td>\n",
              "      <td>0.099622</td>\n",
              "      <td>0.982407</td>\n",
              "      <td>0.018669</td>\n",
              "      <td>0.000645</td>\n",
              "      <td>0.008124</td>\n",
              "      <td>0.000311</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>TensorizedQDA</th>\n",
              "      <td>0.173203</td>\n",
              "      <td>0.051770</td>\n",
              "      <td>0.777053</td>\n",
              "      <td>0.081978</td>\n",
              "      <td>0.982593</td>\n",
              "      <td>0.018443</td>\n",
              "      <td>0.000693</td>\n",
              "      <td>0.012103</td>\n",
              "      <td>0.000227</td>\n",
              "      <td>1.024418</td>\n",
              "      <td>2.071578</td>\n",
              "      <td>1.012255</td>\n",
              "      <td>0.671228</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "               train_mean_ms  train_std_ms  test_mean_ms  test_std_ms  \\\n",
              "model                                                                   \n",
              "QDA                 0.177432      0.065318      1.609726     0.099622   \n",
              "TensorizedQDA       0.173203      0.051770      0.777053     0.081978   \n",
              "\n",
              "               mean_accuracy  train_mem_mean_mb  train_mem_std_mb  \\\n",
              "model                                                               \n",
              "QDA                 0.982407           0.018669          0.000645   \n",
              "TensorizedQDA       0.982593           0.018443          0.000693   \n",
              "\n",
              "               test_mem_mean_mb  test_mem_std_mb  train_speedup  test_speedup  \\\n",
              "model                                                                           \n",
              "QDA                    0.008124         0.000311       1.000000      1.000000   \n",
              "TensorizedQDA          0.012103         0.000227       1.024418      2.071578   \n",
              "\n",
              "               train_mem_reduction  test_mem_reduction  \n",
              "model                                                   \n",
              "QDA                       1.000000            1.000000  \n",
              "TensorizedQDA             1.012255            0.671228  "
            ]
          },
          "execution_count": 201,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# podemos setear un baseline para que fabrique columnas de comparación\n",
        "summ = b.summary(baseline='QDA')\n",
        "\n",
        "summ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 202,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        },
        "id": "z0qeE1gviFLZ",
        "outputId": "26f288da-88c0-4568-d4cb-f3d60bf5045c"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>train_mean_ms</th>\n",
              "      <th>test_mean_ms</th>\n",
              "      <th>mean_accuracy</th>\n",
              "      <th>train_speedup</th>\n",
              "      <th>test_speedup</th>\n",
              "      <th>train_mem_reduction</th>\n",
              "      <th>test_mem_reduction</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>model</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>QDA</th>\n",
              "      <td>0.177432</td>\n",
              "      <td>1.609726</td>\n",
              "      <td>0.982407</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>TensorizedQDA</th>\n",
              "      <td>0.173203</td>\n",
              "      <td>0.777053</td>\n",
              "      <td>0.982593</td>\n",
              "      <td>1.024418</td>\n",
              "      <td>2.071578</td>\n",
              "      <td>1.012255</td>\n",
              "      <td>0.671228</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "               train_mean_ms  test_mean_ms  mean_accuracy  train_speedup  \\\n",
              "model                                                                      \n",
              "QDA                 0.177432      1.609726       0.982407       1.000000   \n",
              "TensorizedQDA       0.173203      0.777053       0.982593       1.024418   \n",
              "\n",
              "               test_speedup  train_mem_reduction  test_mem_reduction  \n",
              "model                                                                 \n",
              "QDA                1.000000             1.000000            1.000000  \n",
              "TensorizedQDA      2.071578             1.012255            0.671228  "
            ]
          },
          "execution_count": 202,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "summ[[\n",
        "    'train_mean_ms', 'test_mean_ms','mean_accuracy',\n",
        "    'train_speedup', 'test_speedup',\n",
        "    'train_mem_reduction', 'test_mem_reduction'\n",
        "]]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EF80Pck2RmaC"
      },
      "source": [
        "# Consigna QDA\n",
        "\n",
        "**Notación**: en general notamos\n",
        "\n",
        "* $k$ la cantidad de clases\n",
        "* $n$ la cantidad de observaciones\n",
        "* $p$ la cantidad de features/variables/predictores\n",
        "\n",
        "**Sugerencia:** combinaciones adecuadas de `transpose`, `stack`, `reshape` y, ocasionalmente, `flatten` y `diagonal` suele ser más que suficiente. Se recomienda **fuertemente* explorar la dimensionalidad de cada elemento antes de implementar las clases."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Tensorización\n",
        "\n",
        "En esta sección nos vamos a ocupar de hacer que el modelo sea más rápido para generar predicciones, observando que incurre en un doble `for` dado que predice en forma individual un escalar para cada observación, para cada clase. Paralelizar ambos vía tensorización suena como una gran vía de mejora de tiempos."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1) Diferencias entre `QDA`y `TensorizedQDA`\n",
        "\n",
        "1. ¿Sobre qué paraleliza `TensorizedQDA`? ¿Sobre las $k$ clases, las $n$ observaciones a predecir, o ambas?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "`QDA` hace un doble ciclo: primero sobre las observaciones y luego sobre las clases.\n",
        "\n",
        "```python\n",
        "for observación x_i:\n",
        "    for clase j:\n",
        "        calcular distancia de x_i a media de clase j usando matriz de covarianza j\n",
        "        usar distancia para obtener verosimilitud\n",
        "```\n",
        "\n",
        "Para cada observacion $i$ y cada clase $j$:\n",
        "- Resta la media de la clase: $(x_i - \\mu_j)$\n",
        "- Aplica la fórmula de distancia de Mahalanobis: $(x_i - \\mu_j)^T \\Sigma_j^{-1} (x_i - \\mu_j)$\n",
        "- Calcula una verosimilitud por clase\n",
        "\n",
        "En cambio, `TensorizedQDA` itera sobre cada observación pero no para cada clase. Vectoriza sobre las clases usando tensores (`np.stack`), evitando el segundo loop.\n",
        "\n",
        "```python\n",
        "for observación x_i:\n",
        "    calcular las verosimilitudes para todas las clases a la vez\n",
        "```\n",
        "\n",
        "Para cada observación $i$:\n",
        "- Calcula las distancias a todas las medias de clase simultáneamente\n",
        "- Evalúa la forma cuadrática para cada clase en paralelo\n",
        "- Usa broadcasting para aplicar todas las $\\Sigma_j^{-1}$ sin necesidad de bucles"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "`TensorizedQDA` paraleliza **sobre las $k$ clases**.\n",
        "\n",
        "Esto se logra al apilar las medias y matrices de covarianza inversas por clase en tensores (`self.tensor_means` y `self.tensor_inv_covs`), lo que permite calcular la log-verosimilitud de una observación respecto a todas las clases simultáneamente utilizando broadcasting.\n",
        "\n",
        "La paralelización ocurre en la linea: `unbiased_x = x - self.tensor_means`. Donde $x$ es una observación de dimensión $(p, 1)$ y `self.tensor_means` es una matriz de shape $(k, p, 1)$. Esta resta se vectoriza automáticamente sobre las k clases.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "2. Analizar los shapes de `tensor_inv_covs` y `tensor_means` y explicar paso a paso cómo es que `TensorizedQDA` llega a predecir lo mismo que `QDA`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- `self.tensor_means`: Tiene shape $(k, p, 1)$, contiene las medias $\\mu_j$ para cada una de las $k$ clases, en dimensión $p$.  --> Cada clase tiene un vector columna que representa la media de cada feature.\n",
        "- `self.tensor_inv_cov`: Tiene shape $(k, p, p)$, contiene las matrices inversas de covarianza $\\Sigma_j^{-1}$ para cada clase. --> Cada clase tiene una matriz cuadrada de tamaño $(p \\times p)$ que representa la inversa de su matriz de covarianza.\n",
        "\n",
        "#### Paso a Paso:\n",
        "1) Stackear los parámetros por clase\n",
        "    - En `_fit_params(X, y)` se calcula la media (`self.means`) y la inversa de la matriz de covarianza (`self.inv_covs`) por clase.\n",
        "    - Luego se apilan en tensores:\n",
        "        - `self.tensor_means`: shape `(k, p, 1)`\n",
        "        - `self.tensor_inv_cov`: shape `(k, p, p)` \n",
        "2) Restar todas las medias al mismo tiempo: `unbiased_x = x - self.tensor_means`\n",
        "    - Se calcula la diferencia entre la observacion $x$ y cada media $\\mu_j$\n",
        "    - Usando *broadcasting*, esta operación se vectoriza automáticamente sobre las $k$ clases.\n",
        "3)  Transformar en vectores fila: `unbiased_x.transpose(0, 2, 1)`\n",
        "    - Transforma los vectores columna en vectores fila. \n",
        "    - Los prepara para que esten en formato correcto para el producto matricial de Mahalanobis.\n",
        "4) Calcular la forma cuadratica vectorizada: `inner_prod = unbiased_x.transpose(0, 2, 1) @ self.tensor_inv_cov @ unbiased_x`\n",
        "    - Obtener un escalar para cada clase con la medida de distancia para cada $j$\n",
        "5) Calcular las log-verosimilitudes de cada clase: `return 0.5 * np.log(LA.det(self.tensor_inv_cov)) - 0.5 * inner_prod.flatten()`\n",
        "    - Mahalanobis: `inner_prod.flatten()`\n",
        "    - Se calcula la log-verosimilitud completa para cada clase simultaneamente.\n",
        "\n",
        "El resultado es el mismo que en QDA porque ambos implementan la misma regla de decisión bayesiana. La diferencia esta en que `QDA`lo hace con un *for* sobre cada clase y `TensorizedQDA` lo hace en paralelo para todas las clases mediante tensores y broadcasting. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2) Optimización\n",
        "\n",
        "Debido a la forma cuadrática de QDA, no se puede predecir para $n$ observaciones en una sola pasada (utilizar $X \\in \\mathbb{R}^{p \\times n}$ en vez de $x \\in \\mathbb{R}^p$) sin pasar por una matriz de $n \\times n$ en donde se computan todas las interacciones entre observaciones. Se puede acceder al resultado recuperando sólo la diagonal de dicha matriz, pero resulta ineficiente en tiempo y (especialmente) en memoria. Aún así, es *posible* que el modelo funcione más rápido."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "3. Implementar el modelo `FasterQDA` (se recomienda heredarlo de `TensorizedQDA`) de manera de eliminar el ciclo for en el método predict."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 203,
      "metadata": {},
      "outputs": [],
      "source": [
        "class FasterQDA(TensorizedQDA):\n",
        "    def _predict_log_conditional(self, X):\n",
        "        \"\"\"\n",
        "        X: shape (p, n) — cada columna es una observación\n",
        "        self.tensor_means: shape (k, p, 1)\n",
        "        self.tensor_inv_cov: shape (k, p, p)\n",
        "        \"\"\"\n",
        "        p, n = X.shape\n",
        "        k = self.tensor_means.shape[0]\n",
        "\n",
        "        # Expandimos X: (p, n) → (1, p, n)\n",
        "        X_exp = X[None, :, :]                         # (1, p, n)\n",
        "        means = self.tensor_means                     # (k, p, 1)\n",
        "        diffs = X_exp - means                         # (k, p, n)\n",
        "\n",
        "        # Multiplicamos: (k, p, n) → (k, n, p)\n",
        "        diffs_T = np.transpose(diffs, axes=(0, 2, 1))  # (k, n, p)\n",
        "\n",
        "        # inv_cov: (k, p, p)\n",
        "        # Primero: A = (k, n, p) @ (k, p, p) → (k, n, p)\n",
        "        A = np.matmul(diffs_T, self.tensor_inv_cov)    # (k, n, p)\n",
        "\n",
        "        # Segundo: M_full = A @ diffs_T.transpose(0,2,1) → (k, n, n)\n",
        "        M_full = np.matmul(A, diffs)                   # (k, n, n)\n",
        "\n",
        "        # Diagonal: (k, n)\n",
        "        dists = np.array([np.diag(M_full[j]) for j in range(k)])  # (k, n)\n",
        "\n",
        "        # Log-verosimilitud\n",
        "        log_det = np.log(np.linalg.det(self.tensor_inv_cov))      # (k,)\n",
        "        log_likelihood = 0.5 * log_det[:, None] - 0.5 * dists      # (k, n)\n",
        "        return log_likelihood\n",
        "\n",
        "    def predict(self, X):\n",
        "        log_cond = self._predict_log_conditional(X)             # (k, n)\n",
        "        posterior = log_cond + self.log_a_priori[:, None]       # (k, n)\n",
        "        return np.argmax(posterior, axis=0).reshape(1, -1)      # (1, n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Diferencias entre `TensorizedQDA` y `FasterQDA`\n",
        "\n",
        "|                       | `TensorizedQDA`                           | `FasterQDA`                                      |\n",
        "|-------------------------------|-------------------------------------------|--------------------------------------------------|\n",
        "| **Paralelización**             | Solo sobre las clases $k$             | Sobre las clases $k$ y las observaciones $n$ |\n",
        "| **Método `predict()`**         | Usa un `for` sobre observaciones (hereda de `BaseBayesianClassifier`). Llama a `_predict_one()` para cada observación | Reescribe `predict()` para vectorizar sobre todas las observaciones.  Calcula todo junto en `_predict_log_conditional(X)` |\n",
        "| **Velocidad de predicción**   | Más lenta al hacer loop sobre $n$             | Más rapida al tener la vectorización completa                    |\n",
        "\n",
        "\n",
        "`FasterQDA`:\n",
        "- Elimina el bucle `for` del método `predict()`\n",
        "- Vectoriza el cálculo de log-verosimilitud para todas las observaciones de una sola vez\n",
        "- Aumenta la eficiencia"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 204,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "FasterQDA (MEM): 100%|██████████| 20/20 [00:00<00:00, 389.84it/s]\n",
            "FasterQDA (TIME): 100%|██████████| 100/100 [00:00<00:00, 3218.74it/s]\n"
          ]
        }
      ],
      "source": [
        "b.bench(FasterQDA)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 206,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>train_mean_ms</th>\n",
              "      <th>test_mean_ms</th>\n",
              "      <th>mean_accuracy</th>\n",
              "      <th>train_mem_mean_mb</th>\n",
              "      <th>test_mem_mean_mb</th>\n",
              "      <th>train_speedup</th>\n",
              "      <th>test_speedup</th>\n",
              "      <th>train_mem_reduction</th>\n",
              "      <th>test_mem_reduction</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>model</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>QDA</th>\n",
              "      <td>0.177432</td>\n",
              "      <td>1.609726</td>\n",
              "      <td>0.982407</td>\n",
              "      <td>0.018669</td>\n",
              "      <td>0.008124</td>\n",
              "      <td>0.976164</td>\n",
              "      <td>0.482724</td>\n",
              "      <td>0.987893</td>\n",
              "      <td>1.489807</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>TensorizedQDA</th>\n",
              "      <td>0.173203</td>\n",
              "      <td>0.777053</td>\n",
              "      <td>0.982593</td>\n",
              "      <td>0.018443</td>\n",
              "      <td>0.012103</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>FasterQDA</th>\n",
              "      <td>0.145618</td>\n",
              "      <td>0.046900</td>\n",
              "      <td>0.985741</td>\n",
              "      <td>0.018152</td>\n",
              "      <td>0.115342</td>\n",
              "      <td>1.189431</td>\n",
              "      <td>16.568310</td>\n",
              "      <td>1.016032</td>\n",
              "      <td>0.104930</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "               train_mean_ms  test_mean_ms  mean_accuracy  train_mem_mean_mb  \\\n",
              "model                                                                          \n",
              "QDA                 0.177432      1.609726       0.982407           0.018669   \n",
              "TensorizedQDA       0.173203      0.777053       0.982593           0.018443   \n",
              "FasterQDA           0.145618      0.046900       0.985741           0.018152   \n",
              "\n",
              "               test_mem_mean_mb  train_speedup  test_speedup  \\\n",
              "model                                                          \n",
              "QDA                    0.008124       0.976164      0.482724   \n",
              "TensorizedQDA          0.012103       1.000000      1.000000   \n",
              "FasterQDA              0.115342       1.189431     16.568310   \n",
              "\n",
              "               train_mem_reduction  test_mem_reduction  \n",
              "model                                                   \n",
              "QDA                       0.987893            1.489807  \n",
              "TensorizedQDA             1.000000            1.000000  \n",
              "FasterQDA                 1.016032            0.104930  "
            ]
          },
          "execution_count": 206,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "summ = b.summary(baseline='TensorizedQDA')\n",
        "\n",
        "summ[[\n",
        "    'train_mean_ms', 'test_mean_ms','mean_accuracy', 'train_mem_mean_mb', 'test_mem_mean_mb',\n",
        "    'train_speedup', 'test_speedup',\n",
        "    'train_mem_reduction', 'test_mem_reduction'\n",
        "]]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Conclusiones:\n",
        "- `FasterQDA` mantiene o mejora la precisión con respecto a `TensorizedQDA`.\n",
        "- Su tiempo de predicción es muchísimo más rápido ($\\Delta ~16.57$ veces más rápido), gracias a la vectorización completa sobre todas las observaciones.\n",
        "- Consume más memoria en test ($\\Delta ~9.53$ veces más memoria), debido a la construcción de matrices de forma $n \\times n$ al calcular las formas cuadráticas de Mahalanobis para todas las clases y observaciones."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "4. Mostrar dónde aparece la mencionada matriz de $n \\times n$, donde $n$ es la cantidad de observaciones a predecir."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "La matriz de $n \\times n$ aparece al calcular la distancia de Mahalanobis entre todas las observaciones $x_i$ y $x_k$, respecto a cada clase $j$.\n",
        "\n",
        "En el modelo `FasterQDA`, se computa la siguiente expresión:\n",
        "\n",
        "`M_full = np.matmul(A, diffs)`\n",
        "\n",
        "Que es equivalente a computar para cada clase $j$:\n",
        "\n",
        "$$\n",
        "M_j = (X - \\mu_j)^T \\Sigma_j^{-1} (X - \\mu_j)\n",
        "$$\n",
        "\n",
        "donde:\n",
        "- $X \\in \\mathbb{R}^{p \\times n}$: contiene las $n$ observaciones como columnas,\n",
        "- $\\mu_j \\in \\mathbb{R}^p$: media de la clase $j$,\n",
        "- $\\Sigma_j^{-1} \\in \\mathbb{R}^{p \\times p}$: matriz inversa de covarianza de la clase $j$,\n",
        "- $M_j \\in \\mathbb{R}^{n \\times n}$: matriz simétrica que contiene en su entrada $(i, k)$ el valor:\n",
        "\n",
        "$$\n",
        "M_j[i, k] = (x_i - \\mu_j)^T \\Sigma_j^{-1} (x_k - \\mu_j)\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "5. Demostrar que\n",
        "$$\n",
        "diag(A \\cdot B) = \\sum_{cols} A \\odot B^T = np.sum(A \\odot B^T, axis=1)\n",
        "$$ \n",
        "\n",
        "es decir, que se puede \"esquivar\" la matriz de $n \\times n$ usando matrices de $n \\times p$. También se puede usar, de forma equivalente,\n",
        "\n",
        "$$\n",
        "np.sum(A^T \\odot B, axis=0).T\n",
        "$$\n",
        "\n",
        "queda a preferencia del alumno cuál usar."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "6. Utilizar la propiedad antes demostrada para reimplementar la predicción del modelo `FasterQDA` de forma eficiente en un nuevo modelo `EfficientQDA`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "7. Comparar la performance de las 4 variantes de QDA implementadas hasta ahora (no Cholesky) ¿Qué se observa? A modo de opinión ¿Se condice con lo esperado?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Cholesky\n",
        "\n",
        "Hasta ahora todos los esfuerzos fueron enfocados en realizar una predicción más rápida. Los tiempos de entrenamiento (teóricos al menos) siguen siendo los mismos o hasta (minúsculamente) peores, dado que todas las mejoras siguen llamando al método `_fit_params` original de `QDA`.\n",
        "\n",
        "La descomposición/factorización de [Cholesky](https://en.wikipedia.org/wiki/Cholesky_decomposition#Statement) permite factorizar una matriz definida positiva $A = LL^T$ donde $L$ es una matriz triangular inferior. En particular, si bien se asume que $p \\ll n$, invertir la matriz de covarianzas $\\Sigma$ para cada clase impone un cuello de botella que podría alivianarse. Teniendo en cuenta que las matrices de covarianza son simétricas y salvo degeneración, definidas positivas, Cholesky como mínimo debería permitir invertir la matriz más rápido.\n",
        "\n",
        "*Nota: observar que calcular* $A^{-1}b$ *equivale a resolver el sistema* $Ax=b$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3) Diferencias entre implementaciones de `QDA_Chol`\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "8. Si una matriz $A$ tiene fact. de Cholesky $A=LL^T$, expresar $A^{-1}$ en términos de $L$. ¿Cómo podría esto ser útil en la forma cuadrática de QDA?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "9. Explicar las diferencias entre `QDA_Chol1`y `QDA` y cómo `QDA_Chol1` llega, paso a paso, hasta las predicciones."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "10. ¿Cuáles son las diferencias entre `QDA_Chol1`, `QDA_Chol2` y `QDA_Chol3`?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "11. Comparar la performance de las 7 variantes de QDA implementadas hasta ahora ¿Qué se observa?¿Hay alguna de las implementaciones de `QDA_Chol` que sea claramente mejor que las demás?¿Alguna que sea peor?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4) Optimización\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "12. Implementar el modelo `TensorizedChol` paralelizando sobre clases/observaciones según corresponda. Se recomienda heredarlo de alguna de las implementaciones de `QDA_Chol`, aunque la elección de cuál de ellas queda a cargo del alumno según lo observado en los benchmarks de puntos anteriores."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "13. Implementar el modelo `EfficientChol` combinando los insights de `EfficientQDA` y `TensorizedChol`. Si se desea, se puede implementar `FasterChol` como ayuda, pero no se contempla para el punto."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "14. Comparar la performance de las 9 variantes de QDA implementadas ¿Qué se observa? A modo de opinión ¿Se condice con lo esperado?"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "CEIA",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.10"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0878ca0785b74f4fa8abce52c184ce33": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_9cb5b6651bac47d0b99c9388def7d2e6",
              "IPY_MODEL_ec4d02ef09c5492ab097ef02b9d3e2fb",
              "IPY_MODEL_ed4b7065efa24e5da4f69c047647c047"
            ],
            "layout": "IPY_MODEL_08dc6e2b043f409f8f8df2b63f6bc152"
          }
        },
        "08dc6e2b043f409f8f8df2b63f6bc152": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1f6553e18f4f460ebbc4b9705cfb9c3d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "26d56661a9e4490c96ef5dd831fa904b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2add89be4e944f4fbd91c5f1d459b5cb": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e0a05e28520841bd95e2aec19da024bd",
              "IPY_MODEL_c406ae72dbf14856971c0ac3ad078062",
              "IPY_MODEL_ca156cc465e1415b8df2a570abfa3ffe"
            ],
            "layout": "IPY_MODEL_40416335cca64bbbad37afc46049363d"
          }
        },
        "2d6622ab061342f280f7fbcbc920bc00": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f87908723b244cd99144a4e1b7be8807",
            "placeholder": "​",
            "style": "IPY_MODEL_568fd1f1b5984675bf1716520a63b7d1",
            "value": " 100/100 [00:00&lt;00:00, 180.63it/s]"
          }
        },
        "38dcdaf521fe4043b7bc5fac762867e1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3fad52f5ac204004afac2df2b55cbc7b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ba742120ade4495d9f75b066c43b6ffb",
            "max": 100,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e38ba66c86764d578b97aa127e0cd9ce",
            "value": 100
          }
        },
        "40416335cca64bbbad37afc46049363d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "43ef5277fff74410bb85f09e64c37cfe": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "44cc54751afe4e618d81e92e2e64ca58": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4531ea3385d34454865e3c6ced188122": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4965b54e63794e0b80c19c150b072eda": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "4c09e94a9e494e6daa6f59c23b8bcb4f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "568fd1f1b5984675bf1716520a63b7d1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "68a99e1ecb9741f688cb33bbfc19d5d2": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6bbe1b2d6f3846168244aca7ecf761f0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_eb6e454a891d493b88fe82442a9288e2",
              "IPY_MODEL_3fad52f5ac204004afac2df2b55cbc7b",
              "IPY_MODEL_2d6622ab061342f280f7fbcbc920bc00"
            ],
            "layout": "IPY_MODEL_e7cdc4320bff45f6acf901b8bd9bdf1a"
          }
        },
        "74859eadc1ef4090a07e851f5949f4d6": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9cb5b6651bac47d0b99c9388def7d2e6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f7bc1a202cde4351a6f98ae19393cf94",
            "placeholder": "​",
            "style": "IPY_MODEL_caea67fec94c45a097cae720b582c68d",
            "value": "QDA (TIME): 100%"
          }
        },
        "9e38484f037e4f86be6b768459404767": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f1cdc4724c6d441cbebca60f1bb11aea",
            "max": 20,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_4965b54e63794e0b80c19c150b072eda",
            "value": 20
          }
        },
        "a5957e8514974187838fc2d43d548433": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a9bc2a66f8a84516a44d8d3ad8e885e2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ab3b1739c3bf4f3ba399e3a60d043ec8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b2d747a3bfdd4f4797f7e4b01964aa7b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ba742120ade4495d9f75b066c43b6ffb": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bf6f0511eeca467c870c6a261712ca7e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c136c5327b804c74b0582ed04c8825dc": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c406ae72dbf14856971c0ac3ad078062": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_43ef5277fff74410bb85f09e64c37cfe",
            "max": 20,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d7276daf0b654112aab411ae3f14649f",
            "value": 20
          }
        },
        "ca156cc465e1415b8df2a570abfa3ffe": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_68a99e1ecb9741f688cb33bbfc19d5d2",
            "placeholder": "​",
            "style": "IPY_MODEL_cec5bcb756694268abfc4eefad72f758",
            "value": " 20/20 [00:00&lt;00:00, 37.71it/s]"
          }
        },
        "caea67fec94c45a097cae720b582c68d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "cec5bcb756694268abfc4eefad72f758": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d670b24d966f4de1872524684ea98840": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4c09e94a9e494e6daa6f59c23b8bcb4f",
            "placeholder": "​",
            "style": "IPY_MODEL_ab3b1739c3bf4f3ba399e3a60d043ec8",
            "value": " 20/20 [00:00&lt;00:00, 88.29it/s]"
          }
        },
        "d7276daf0b654112aab411ae3f14649f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e0a05e28520841bd95e2aec19da024bd": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4531ea3385d34454865e3c6ced188122",
            "placeholder": "​",
            "style": "IPY_MODEL_a9bc2a66f8a84516a44d8d3ad8e885e2",
            "value": "QDA (MEM): 100%"
          }
        },
        "e3704aaa731c464a8d6c63c39d0b167a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ee002c5199874f8db461bb1631f198ac",
              "IPY_MODEL_9e38484f037e4f86be6b768459404767",
              "IPY_MODEL_d670b24d966f4de1872524684ea98840"
            ],
            "layout": "IPY_MODEL_74859eadc1ef4090a07e851f5949f4d6"
          }
        },
        "e38ba66c86764d578b97aa127e0cd9ce": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e7cdc4320bff45f6acf901b8bd9bdf1a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "eb6e454a891d493b88fe82442a9288e2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bf6f0511eeca467c870c6a261712ca7e",
            "placeholder": "​",
            "style": "IPY_MODEL_26d56661a9e4490c96ef5dd831fa904b",
            "value": "TensorizedQDA (TIME): 100%"
          }
        },
        "ec4d02ef09c5492ab097ef02b9d3e2fb": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c136c5327b804c74b0582ed04c8825dc",
            "max": 100,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b2d747a3bfdd4f4797f7e4b01964aa7b",
            "value": 100
          }
        },
        "ed4b7065efa24e5da4f69c047647c047": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a5957e8514974187838fc2d43d548433",
            "placeholder": "​",
            "style": "IPY_MODEL_38dcdaf521fe4043b7bc5fac762867e1",
            "value": " 100/100 [00:00&lt;00:00, 114.33it/s]"
          }
        },
        "ee002c5199874f8db461bb1631f198ac": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1f6553e18f4f460ebbc4b9705cfb9c3d",
            "placeholder": "​",
            "style": "IPY_MODEL_44cc54751afe4e618d81e92e2e64ca58",
            "value": "TensorizedQDA (MEM): 100%"
          }
        },
        "f1cdc4724c6d441cbebca60f1bb11aea": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f7bc1a202cde4351a6f98ae19393cf94": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f87908723b244cd99144a4e1b7be8807": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
