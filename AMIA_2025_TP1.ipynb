{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "    <h1> <font style=\"bold\"> Trabajo Práctico </font></h1>\n",
    "    <h2><font style=\"bold\">LDA/QDA y optimización matemática de modelos</font></h2>\n",
    "    <h3><font style=\"bold\">Abril Noguera - Pablo Brahim - Fermin Rodriguez - Kevin Pennington</font></h3>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4kL_4etdeizy"
   },
   "source": [
    "# Intro teórica\n",
    "\n",
    "## Definición: Clasificador Bayesiano\n",
    "\n",
    "Sean $k$ poblaciones, $x \\in \\mathbb{R}^p$ puede pertenecer a cualquiera $g \\in \\mathcal{G}$ de ellas. Bajo un esquema bayesiano, se define entonces $\\pi_j \\doteq P(G = j)$ la probabilidad *a priori* de que $X$ pertenezca a la clase *j*, y se **asume conocida** la distribución condicional de cada observable dado su clase $f_j \\doteq f_{X|G=j}$.\n",
    "\n",
    "De esta manera dicha probabilidad *a posteriori* resulta\n",
    "$$\n",
    "P(G|_{X=x} = j) = \\frac{f_{X|G=j}(x) \\cdot p_G(j)}{f_X(x)} \\propto f_j(x) \\cdot \\pi_j\n",
    "$$\n",
    "\n",
    "La regla de decisión de Bayes es entonces\n",
    "$$\n",
    "H(x) \\doteq \\arg \\max_{g \\in \\mathcal{G}} \\{ P(G|_{X=x} = j) \\} = \\arg \\max_{g \\in \\mathcal{G}} \\{ f_j(x) \\cdot \\pi_j \\}\n",
    "$$\n",
    "\n",
    "es decir, se predice a $x$ como perteneciente a la población $j$ cuya probabilidad a posteriori es máxima.\n",
    "\n",
    "*Ojo, a no desesperar! $\\pi_j$ no es otra cosa que una constante prefijada, y $f_j$ es, en su esencia, un campo escalar de $x$ a simplemente evaluar.*\n",
    "\n",
    "## Distribución condicional\n",
    "\n",
    "Para los clasificadores de discriminante cuadrático y lineal (QDA/LDA) se asume que $X|_{G=j} \\sim \\mathcal{N}_p(\\mu_j, \\Sigma_j)$, es decir, se asume que cada población sigue una distribución normal.\n",
    "\n",
    "Por definición, se tiene entonces que para una clase $j$:\n",
    "$$\n",
    "f_j(x) = \\frac{1}{(2 \\pi)^\\frac{p}{2} \\cdot |\\Sigma_j|^\\frac{1}{2}} e^{- \\frac{1}{2}(x-\\mu_j)^T \\Sigma_j^{-1} (x- \\mu_j)}\n",
    "$$\n",
    "\n",
    "Aplicando logaritmo (que al ser una función estrictamente creciente no afecta el cálculo de máximos/mínimos), queda algo mucho más práctico de trabajar:\n",
    "\n",
    "$$\n",
    "\\log{f_j(x)} = -\\frac{1}{2}\\log |\\Sigma_j| - \\frac{1}{2} (x-\\mu_j)^T \\Sigma_j^{-1} (x- \\mu_j) + C\n",
    "$$\n",
    "\n",
    "Observar que en este caso $C=-\\frac{p}{2} \\log(2\\pi)$, pero no se tiene en cuenta ya que al tener una constante aditiva en todas las clases, no afecta al cálculo del máximo.\n",
    "\n",
    "## LDA\n",
    "\n",
    "En el caso de LDA se hace una suposición extra, que es $X|_{G=j} \\sim \\mathcal{N}_p(\\mu_j, \\Sigma)$, es decir que las poblaciones no sólo siguen una distribución normal sino que son de igual matriz de covarianzas. Reemplazando arriba se obtiene entonces:\n",
    "\n",
    "$$\n",
    "\\log{f_j(x)} =  -\\frac{1}{2}\\log |\\Sigma| - \\frac{1}{2} (x-\\mu_j)^T \\Sigma^{-1} (x- \\mu_j) + C\n",
    "$$\n",
    "\n",
    "Ahora, como $-\\frac{1}{2}\\log |\\Sigma|$ es común a todas las clases se puede incorporar a la constante aditiva y, distribuyendo y reagrupando términos sobre $(x-\\mu_j)^T \\Sigma^{-1} (x- \\mu_j)$ se obtiene finalmente:\n",
    "\n",
    "$$\n",
    "\\log{f_j(x)} =  \\mu_j^T \\Sigma^{-1} (x- \\frac{1}{2} \\mu_j) + C'\n",
    "$$\n",
    "\n",
    "## Entrenamiento/Ajuste\n",
    "\n",
    "Obsérvese que para ambos modelos, ajustarlos a los datos implica estimar los parámetros $(\\mu_j, \\Sigma_j) \\; \\forall j = 1, \\dots, k$ en el caso de QDA, y $(\\mu_j, \\Sigma)$ para LDA.\n",
    "\n",
    "Estos parámetros se estiman por máxima verosimilitud, de manera que los estimadores resultan:\n",
    "\n",
    "* $\\hat{\\mu}_j = \\bar{x}_j$ el promedio de los $x$ de la clase *j*\n",
    "* $\\hat{\\Sigma}_j = s^2_j$ la matriz de covarianzas estimada para cada clase *j*\n",
    "* $\\hat{\\pi}_j = f_{R_j} = \\frac{n_j}{n}$ la frecuencia relativa de la clase *j* en la muestra\n",
    "* $\\hat{\\Sigma} = \\frac{1}{n} \\sum_{j=1}^k n_j \\cdot s^2_j$ el promedio ponderado (por frecs. relativas) de las matrices de covarianzas de todas las clases. *Observar que se utiliza el estimador de MV y no el insesgado*\n",
    "\n",
    "Es importante notar que si bien todos los $\\mu, \\Sigma$ deben ser estimados, la distribución *a priori* puede no inferirse de los datos sino asumirse previamente, utilizándose como entrada del modelo.\n",
    "\n",
    "## Predicción\n",
    "\n",
    "Para estos modelos, al igual que para cualquier clasificador Bayesiano del tipo antes visto, la estimación de la clase es por método *plug-in* sobre la regla de decisión $H(x)$, es decir devolver la clase que maximiza $\\hat{f}_j(x) \\cdot \\hat{\\pi}_j$, o lo que es lo mismo $\\log\\hat{f}_j(x) + \\log\\hat{\\pi}_j$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IV8OF-SlPHbD"
   },
   "source": [
    "# Código provisto\n",
    "\n",
    "Con el fin de no retrasar al alumno con cuestiones estructurales y/o secundarias al tema que se pretende tratar, se provee una base de código que **no es obligatoria de usar** pero se asume que resulta resulta beneficiosa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "PrDdJRypNB-y"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import numpy.linalg as LA\n",
    "from scipy.linalg import cholesky, solve_triangular\n",
    "from scipy.linalg.lapack import dtrtri"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5cPL33WIN2HA"
   },
   "source": [
    "## Base code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "ewg5e0hsNTQC"
   },
   "outputs": [],
   "source": [
    "class BaseBayesianClassifier:\n",
    "  def __init__(self):\n",
    "    pass\n",
    "\n",
    "  def _estimate_a_priori(self, y):\n",
    "    a_priori = np.bincount(y.flatten().astype(int)) / y.size\n",
    "    # Q3: para que sirve bincount? --> cuenta cuantas veces aparece cada clase en y.\n",
    "    return np.log(a_priori)\n",
    "\n",
    "  def _fit_params(self, X, y):\n",
    "    # estimate all needed parameters for given model\n",
    "    raise NotImplementedError()\n",
    "\n",
    "  def _predict_log_conditional(self, x, class_idx):\n",
    "    # predict the log(P(x|G=class_idx)), the log of the conditional probability of x given the class\n",
    "    # this should depend on the model used\n",
    "    raise NotImplementedError()\n",
    "\n",
    "  def fit(self, X, y, a_priori=None):\n",
    "    # if it's needed, estimate a priori probabilities\n",
    "    self.log_a_priori = self._estimate_a_priori(y) if a_priori is None else np.log(a_priori)\n",
    "\n",
    "    # now that everything else is in place, estimate all needed parameters for given model\n",
    "    self._fit_params(X, y)\n",
    "    # Q4: por que el _fit_params va al final? no se puede mover a, por ejemplo, antes de la priori? --> \n",
    "\n",
    "  def predict(self, X):\n",
    "    # this is actually an individual prediction encased in a for-loop\n",
    "    m_obs = X.shape[1]\n",
    "    y_hat = np.empty(m_obs, dtype=int)\n",
    "\n",
    "    for i in range(m_obs):\n",
    "      y_hat[i] = self._predict_one(X[:,i].reshape(-1,1))\n",
    "\n",
    "    # return prediction as a row vector (matching y)\n",
    "    return y_hat.reshape(1,-1)\n",
    "\n",
    "  def _predict_one(self, x):\n",
    "    # calculate all log posteriori probabilities (actually, +C)\n",
    "    log_posteriori = [ log_a_priori_i + self._predict_log_conditional(x, idx) for idx, log_a_priori_i\n",
    "                  in enumerate(self.log_a_priori) ]\n",
    "\n",
    "    # return the class that has maximum a posteriori probability\n",
    "    return np.argmax(log_posteriori)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "Rz2FC7A5NUpN"
   },
   "outputs": [],
   "source": [
    "class QDA(BaseBayesianClassifier):\n",
    "\n",
    "  def _fit_params(self, X, y):\n",
    "    # estimate each covariance matrix\n",
    "    self.inv_covs = [LA.inv(np.cov(X[:,y.flatten()==idx], bias=True))\n",
    "                      for idx in range(len(self.log_a_priori))]\n",
    "    # Q5: por que hace falta el flatten y no se puede directamente X[:,y==idx]?\n",
    "    # Q6: por que se usa bias=True en vez del default bias=False?\n",
    "    self.means = [X[:,y.flatten()==idx].mean(axis=1, keepdims=True)\n",
    "                  for idx in range(len(self.log_a_priori))]\n",
    "    # Q7: que hace axis=1? por que no axis=0?\n",
    "\n",
    "  def _predict_log_conditional(self, x, class_idx):\n",
    "    # predict the log(P(x|G=class_idx)), the log of the conditional probability of x given the class\n",
    "    # this should depend on the model used\n",
    "    inv_cov = self.inv_covs[class_idx]\n",
    "    unbiased_x =  x - self.means[class_idx]\n",
    "    return 0.5*np.log(LA.det(inv_cov)) -0.5 * unbiased_x.T @ inv_cov @ unbiased_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "9lZbID0WNV1Y"
   },
   "outputs": [],
   "source": [
    "class TensorizedQDA(QDA):\n",
    "\n",
    "    def _fit_params(self, X, y):\n",
    "        # ask plain QDA to fit params\n",
    "        super()._fit_params(X,y)\n",
    "\n",
    "        # stack onto new dimension\n",
    "        self.tensor_inv_cov = np.stack(self.inv_covs) # → shape: (k, p, p)\n",
    "        self.tensor_means = np.stack(self.means) # → shape: (k, p, 1)\n",
    "\n",
    "    def _predict_log_conditionals(self,x):\n",
    "        unbiased_x = x - self.tensor_means # → shape: (k, p, 1)\n",
    "        inner_prod = unbiased_x.transpose(0,2,1) @ self.tensor_inv_cov @ unbiased_x # → shape: (k, 1, 1)\n",
    "\n",
    "        return 0.5*np.log(LA.det(self.tensor_inv_cov)) - 0.5 * inner_prod.flatten() # → shape: (k, )\n",
    "\n",
    "    def _predict_one(self, x):\n",
    "        # return the class that has maximum a posteriori probability\n",
    "        return np.argmax(self.log_a_priori + self._predict_log_conditionals(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "i-WGGi_sQ-pT"
   },
   "outputs": [],
   "source": [
    "class QDA_Chol1(BaseBayesianClassifier):\n",
    "  def _fit_params(self, X, y):\n",
    "    self.L_invs = [\n",
    "        LA.inv(cholesky(np.cov(X[:,y.flatten()==idx], bias=True), lower=True))\n",
    "        for idx in range(len(self.log_a_priori))\n",
    "    ]\n",
    "\n",
    "    self.means = [X[:,y.flatten()==idx].mean(axis=1, keepdims=True)\n",
    "                  for idx in range(len(self.log_a_priori))]\n",
    "\n",
    "  def _predict_log_conditional(self, x, class_idx):\n",
    "    L_inv = self.L_invs[class_idx]\n",
    "    unbiased_x =  x - self.means[class_idx]\n",
    "\n",
    "    y = L_inv @ unbiased_x\n",
    "\n",
    "    return np.log(L_inv.diagonal().prod()) -0.5 * (y**2).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "i5DNLtYbQsHi"
   },
   "outputs": [],
   "source": [
    "class QDA_Chol2(BaseBayesianClassifier):\n",
    "  def _fit_params(self, X, y):\n",
    "    self.Ls = [\n",
    "        cholesky(np.cov(X[:,y.flatten()==idx], bias=True), lower=True)\n",
    "        for idx in range(len(self.log_a_priori))\n",
    "    ]\n",
    "\n",
    "    self.means = [X[:,y.flatten()==idx].mean(axis=1, keepdims=True)\n",
    "                  for idx in range(len(self.log_a_priori))]\n",
    "\n",
    "  def _predict_log_conditional(self, x, class_idx):\n",
    "    L = self.Ls[class_idx]\n",
    "    unbiased_x =  x - self.means[class_idx]\n",
    "\n",
    "    y = solve_triangular(L, unbiased_x, lower=True)\n",
    "\n",
    "    return -np.log(L.diagonal().prod()) -0.5 * (y**2).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "v0dRvYVQRCgc"
   },
   "outputs": [],
   "source": [
    "class QDA_Chol3(BaseBayesianClassifier):\n",
    "  def _fit_params(self, X, y):\n",
    "    self.L_invs = [\n",
    "        dtrtri(cholesky(np.cov(X[:,y.flatten()==idx], bias=True), lower=True), lower=1)[0]\n",
    "        for idx in range(len(self.log_a_priori))\n",
    "    ]\n",
    "\n",
    "    self.means = [X[:,y.flatten()==idx].mean(axis=1, keepdims=True)\n",
    "                  for idx in range(len(self.log_a_priori))]\n",
    "\n",
    "  def _predict_log_conditional(self, x, class_idx):\n",
    "    L_inv = self.L_invs[class_idx]\n",
    "    unbiased_x =  x - self.means[class_idx]\n",
    "\n",
    "    y = L_inv @ unbiased_x\n",
    "\n",
    "    return np.log(L_inv.diagonal().prod()) -0.5 * (y**2).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JCtrHQDuN6R4"
   },
   "source": [
    "## Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "rasInBMFNzUH"
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris, fetch_openml, load_wine\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def get_iris_dataset():\n",
    "  data = load_iris()\n",
    "  X_full = data.data\n",
    "  y_full = np.array([data.target_names[y] for y in data.target.reshape(-1,1)])\n",
    "  return X_full, y_full\n",
    "\n",
    "def get_penguins_dataset():\n",
    "    # get data\n",
    "    df, tgt = fetch_openml(name=\"penguins\", return_X_y=True, as_frame=True, parser='auto')\n",
    "\n",
    "    # drop non-numeric columns\n",
    "    df.drop(columns=[\"island\",\"sex\"], inplace=True)\n",
    "\n",
    "    # drop rows with missing values\n",
    "    mask = df.isna().sum(axis=1) == 0\n",
    "    df = df[mask]\n",
    "    tgt = tgt[mask]\n",
    "\n",
    "    return df.values, tgt.to_numpy().reshape(-1,1)\n",
    "\n",
    "def get_wine_dataset():\n",
    "    # get data\n",
    "    data = load_wine()\n",
    "    X_full = data.data\n",
    "    y_full = np.array([data.target_names[y] for y in data.target.reshape(-1,1)])\n",
    "    return X_full, y_full\n",
    "\n",
    "def get_letters_dataset():\n",
    "    # get data\n",
    "    letter = fetch_openml('letter', version=1, as_frame=False)\n",
    "    return letter.data, letter.target.reshape(-1,1)\n",
    "\n",
    "def label_encode(y_full):\n",
    "    return LabelEncoder().fit_transform(y_full.flatten()).reshape(y_full.shape)\n",
    "\n",
    "def split_transpose(X, y, test_size, random_state):\n",
    "    # X_train, X_test, y_train, y_test but all transposed\n",
    "    return [elem.T for elem in train_test_split(X, y, test_size=test_size, random_state=random_state)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ybPkuBdDN42P"
   },
   "source": [
    "## Benchmarking\n",
    "\n",
    "Nota: esta clase fue creada bastante rápido y no pretende ser una plataforma súper confiable sobre la que basarse, sino más bien una herramienta simple con la que poder medir varios runs y agregar la información.\n",
    "\n",
    "En forma rápida, `warmup` es la cantidad de runs para warmup, `mem_runs` es la cantidad de runs en las que se mide el pico de uso de RAM y `n_runs` es la cantidad de runs en las que se miden tiempos.\n",
    "\n",
    "La razón por la que se separan es que medir memoria hace ~2.5x más lento cada run, pero al mismo tiempo se estabiliza mucho más rápido.\n",
    "\n",
    "**Importante:** tener en cuenta que los modelos que predicen en batch (usan `predict` directamente) deberían consumir, como mínimo, $n$ veces la memoria de los que predicen por observación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "nO4Py3CeNpKu"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "from tqdm import tqdm\n",
    "from numpy.random import RandomState\n",
    "import tracemalloc\n",
    "\n",
    "RNG_SEED = 6553\n",
    "\n",
    "class Benchmark:\n",
    "    def __init__(self, X, y, n_runs=1000, warmup=100, mem_runs=100, test_sz=0.3, rng_seed=RNG_SEED, same_splits=True):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.n = n_runs\n",
    "        self.warmup = warmup\n",
    "        self.mem_runs = mem_runs\n",
    "        self.test_sz = test_sz\n",
    "        self.det = same_splits\n",
    "        if self.det:\n",
    "            self.rng_seed = rng_seed\n",
    "        else:\n",
    "            self.rng = RandomState(rng_seed)\n",
    "\n",
    "        self.data = dict()\n",
    "\n",
    "        print(\"Benching params:\")\n",
    "        print(\"Total runs:\",self.warmup+self.mem_runs+self.n)\n",
    "        print(\"Warmup runs:\",self.warmup)\n",
    "        print(\"Peak Memory usage runs:\", self.mem_runs)\n",
    "        print(\"Running time runs:\", self.n)\n",
    "        approx_test_sz = int(self.y.size * self.test_sz)\n",
    "        print(\"Train size rows (approx):\",self.y.size - approx_test_sz)\n",
    "        print(\"Test size rows (approx):\",approx_test_sz)\n",
    "        print(\"Test size fraction:\",self.test_sz)\n",
    "\n",
    "    def bench(self, model_class, **kwargs):\n",
    "        name = model_class.__name__\n",
    "        time_data = np.empty((self.n, 3), dtype=float)  # train_time, test_time, accuracy\n",
    "        mem_data = np.empty((self.mem_runs, 2), dtype=float)  # train_peak_mem, test_peak_mem\n",
    "        rng = RandomState(self.rng_seed) if self.det else self.rng\n",
    "\n",
    "\n",
    "        for i in range(self.warmup):\n",
    "            # Instantiate model with error check for unsupported parameters\n",
    "            model = model_class(**kwargs)\n",
    "\n",
    "            # Generate current train-test split\n",
    "            X_train, X_test, y_train, y_test = split_transpose(\n",
    "                self.X, self.y,\n",
    "                test_size=self.test_sz,\n",
    "                random_state=rng\n",
    "            )\n",
    "            # Run training and prediction (timing or memory measurement not recorded)\n",
    "            model.fit(X_train, y_train)\n",
    "            model.predict(X_test)\n",
    "\n",
    "        for i in tqdm(range(self.mem_runs), total=self.mem_runs, desc=f\"{name} (MEM)\"):\n",
    "\n",
    "            model = model_class(**kwargs)\n",
    "\n",
    "            X_train, X_test, y_train, y_test = split_transpose(\n",
    "                self.X, self.y,\n",
    "                test_size=self.test_sz,\n",
    "                random_state=rng\n",
    "            )\n",
    "\n",
    "            tracemalloc.start()\n",
    "\n",
    "            t1 = time.perf_counter()\n",
    "            model.fit(X_train, y_train)\n",
    "            t2 = time.perf_counter()\n",
    "\n",
    "            _, train_peak = tracemalloc.get_traced_memory()\n",
    "            tracemalloc.reset_peak()\n",
    "\n",
    "            model.predict(X_test)\n",
    "            t3 = time.perf_counter()\n",
    "            _, test_peak = tracemalloc.get_traced_memory()\n",
    "            tracemalloc.stop()\n",
    "\n",
    "            mem_data[i,] = (\n",
    "                train_peak / (1024 * 1024),\n",
    "                test_peak / (1024 * 1024)\n",
    "            )\n",
    "\n",
    "        for i in tqdm(range(self.n), total=self.n, desc=f\"{name} (TIME)\"):\n",
    "            model = model_class(**kwargs)\n",
    "\n",
    "            X_train, X_test, y_train, y_test = split_transpose(\n",
    "                self.X, self.y,\n",
    "                test_size=self.test_sz,\n",
    "                random_state=rng\n",
    "            )\n",
    "\n",
    "            t1 = time.perf_counter()\n",
    "            model.fit(X_train, y_train)\n",
    "            t2 = time.perf_counter()\n",
    "            preds = model.predict(X_test)\n",
    "            t3 = time.perf_counter()\n",
    "\n",
    "            time_data[i,] = (\n",
    "                (t2 - t1) * 1000,\n",
    "                (t3 - t2) * 1000,\n",
    "                (y_test.flatten() == preds.flatten()).mean()\n",
    "            )\n",
    "\n",
    "        self.data[name] = (time_data, mem_data)\n",
    "\n",
    "    def summary(self, baseline=None):\n",
    "        aux = []\n",
    "        for name, (time_data, mem_data) in self.data.items():\n",
    "            result = {\n",
    "                'model': name,\n",
    "                'train_mean_ms': time_data[:, 0].mean(),\n",
    "                'train_std_ms': time_data[:, 0].std(),\n",
    "                'test_mean_ms': time_data[:, 1].mean(),\n",
    "                'test_std_ms': time_data[:, 1].std(),\n",
    "                'mean_accuracy': time_data[:, 2].mean(),\n",
    "                'train_mem_mean_mb': mem_data[:, 0].mean(),\n",
    "                'train_mem_std_mb': mem_data[:, 0].std(),\n",
    "                'test_mem_mean_mb': mem_data[:, 1].mean(),\n",
    "                'test_mem_std_mb': mem_data[:, 1].std()\n",
    "            }\n",
    "            aux.append(result)\n",
    "        df = pd.DataFrame(aux).set_index('model')\n",
    "\n",
    "        if baseline is not None and baseline in self.data:\n",
    "            df['train_speedup'] = df.loc[baseline, 'train_mean_ms'] / df['train_mean_ms']\n",
    "            df['test_speedup'] = df.loc[baseline, 'test_mean_ms'] / df['test_mean_ms']\n",
    "            df['train_mem_reduction'] = df.loc[baseline, 'train_mem_mean_mb'] / df['train_mem_mean_mb']\n",
    "            df['test_mem_reduction'] = df.loc[baseline, 'test_mem_mean_mb'] / df['test_mem_mean_mb']\n",
    "        return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mb5VEpEugFXW"
   },
   "source": [
    "## Ejemplo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fLyr4-hdgJ7e",
    "outputId": "bfa9623f-2baf-4735-96b5-c714b244b8da"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((178, 13), (178, 1))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# levantamos el dataset Wine, que tiene 13 features y 178 observaciones en total\n",
    "X_full, y_full = get_wine_dataset()\n",
    "\n",
    "X_full.shape, y_full.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZxQlFUSbgYHQ",
    "outputId": "dd396038-8bab-4ebd-b981-752526d8c98c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([['class_0'],\n",
       "        ['class_0'],\n",
       "        ['class_0'],\n",
       "        ['class_0'],\n",
       "        ['class_0']], dtype='<U7'),\n",
       " array([[0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0]]))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# encodeamos a número las clases\n",
    "y_full_encoded = label_encode(y_full)\n",
    "\n",
    "y_full[:5], y_full_encoded[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pSBNNUOmgtsI",
    "outputId": "d29b11de-b5a9-4fa3-f009-d8780104fa64"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benching params:\n",
      "Total runs: 140\n",
      "Warmup runs: 20\n",
      "Peak Memory usage runs: 20\n",
      "Running time runs: 100\n",
      "Train size rows (approx): 125\n",
      "Test size rows (approx): 53\n",
      "Test size fraction: 0.3\n"
     ]
    }
   ],
   "source": [
    "# generamos el benchmark\n",
    "# observar que son valores muy bajos de runs para que corra rápido ahora\n",
    "b = Benchmark(\n",
    "    X_full, y_full_encoded,\n",
    "    n_runs = 100,\n",
    "    warmup = 20,\n",
    "    mem_runs = 20,\n",
    "    test_sz = 0.3,\n",
    "    same_splits = False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 81,
     "referenced_widgets": [
      "2add89be4e944f4fbd91c5f1d459b5cb",
      "e0a05e28520841bd95e2aec19da024bd",
      "c406ae72dbf14856971c0ac3ad078062",
      "ca156cc465e1415b8df2a570abfa3ffe",
      "40416335cca64bbbad37afc46049363d",
      "4531ea3385d34454865e3c6ced188122",
      "a9bc2a66f8a84516a44d8d3ad8e885e2",
      "43ef5277fff74410bb85f09e64c37cfe",
      "d7276daf0b654112aab411ae3f14649f",
      "68a99e1ecb9741f688cb33bbfc19d5d2",
      "cec5bcb756694268abfc4eefad72f758",
      "0878ca0785b74f4fa8abce52c184ce33",
      "9cb5b6651bac47d0b99c9388def7d2e6",
      "ec4d02ef09c5492ab097ef02b9d3e2fb",
      "ed4b7065efa24e5da4f69c047647c047",
      "08dc6e2b043f409f8f8df2b63f6bc152",
      "f7bc1a202cde4351a6f98ae19393cf94",
      "caea67fec94c45a097cae720b582c68d",
      "c136c5327b804c74b0582ed04c8825dc",
      "b2d747a3bfdd4f4797f7e4b01964aa7b",
      "a5957e8514974187838fc2d43d548433",
      "38dcdaf521fe4043b7bc5fac762867e1"
     ]
    },
    "id": "zUciOjazhUu5",
    "outputId": "91fc0889-7a87-418e-9950-88371c912be9"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "QDA (MEM): 100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 20/20 [00:00<00:00, 116.03it/s]\n",
      "QDA (TIME): 100%|███████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:00<00:00, 271.82it/s]\n"
     ]
    }
   ],
   "source": [
    "# bencheamos un par\n",
    "to_bench = [QDA]\n",
    "\n",
    "for model in to_bench:\n",
    "    b.bench(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 81,
     "referenced_widgets": [
      "e3704aaa731c464a8d6c63c39d0b167a",
      "ee002c5199874f8db461bb1631f198ac",
      "9e38484f037e4f86be6b768459404767",
      "d670b24d966f4de1872524684ea98840",
      "74859eadc1ef4090a07e851f5949f4d6",
      "1f6553e18f4f460ebbc4b9705cfb9c3d",
      "44cc54751afe4e618d81e92e2e64ca58",
      "f1cdc4724c6d441cbebca60f1bb11aea",
      "4965b54e63794e0b80c19c150b072eda",
      "4c09e94a9e494e6daa6f59c23b8bcb4f",
      "ab3b1739c3bf4f3ba399e3a60d043ec8",
      "6bbe1b2d6f3846168244aca7ecf761f0",
      "eb6e454a891d493b88fe82442a9288e2",
      "3fad52f5ac204004afac2df2b55cbc7b",
      "2d6622ab061342f280f7fbcbc920bc00",
      "e7cdc4320bff45f6acf901b8bd9bdf1a",
      "bf6f0511eeca467c870c6a261712ca7e",
      "26d56661a9e4490c96ef5dd831fa904b",
      "ba742120ade4495d9f75b066c43b6ffb",
      "e38ba66c86764d578b97aa127e0cd9ce",
      "f87908723b244cd99144a4e1b7be8807",
      "568fd1f1b5984675bf1716520a63b7d1"
     ]
    },
    "id": "wpPhSSCNhlvG",
    "outputId": "a5302dc3-d947-47db-d96d-cf7a98152b0e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TensorizedQDA (MEM): 100%|████████████████████████████████████████████████████████████████████████████████████████| 20/20 [00:00<00:00, 245.62it/s]\n",
      "TensorizedQDA (TIME): 100%|█████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:00<00:00, 505.94it/s]\n"
     ]
    }
   ],
   "source": [
    "# como es una clase, podemos seguir bencheando más después\n",
    "b.bench(TensorizedQDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 163
    },
    "id": "bZ5-vowshr5c",
    "outputId": "f17bc091-0cf5-42b9-cd9a-1e3d61e824b0"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train_mean_ms</th>\n",
       "      <th>train_std_ms</th>\n",
       "      <th>test_mean_ms</th>\n",
       "      <th>test_std_ms</th>\n",
       "      <th>mean_accuracy</th>\n",
       "      <th>train_mem_mean_mb</th>\n",
       "      <th>train_mem_std_mb</th>\n",
       "      <th>test_mem_mean_mb</th>\n",
       "      <th>test_mem_std_mb</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>QDA</th>\n",
       "      <td>0.362195</td>\n",
       "      <td>0.115624</td>\n",
       "      <td>2.979891</td>\n",
       "      <td>0.449799</td>\n",
       "      <td>0.982407</td>\n",
       "      <td>0.018380</td>\n",
       "      <td>0.000724</td>\n",
       "      <td>0.008446</td>\n",
       "      <td>0.001775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TensorizedQDA</th>\n",
       "      <td>0.377502</td>\n",
       "      <td>0.087030</td>\n",
       "      <td>1.270504</td>\n",
       "      <td>0.166855</td>\n",
       "      <td>0.982593</td>\n",
       "      <td>0.018187</td>\n",
       "      <td>0.000661</td>\n",
       "      <td>0.012032</td>\n",
       "      <td>0.000070</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               train_mean_ms  train_std_ms  test_mean_ms  test_std_ms  \\\n",
       "model                                                                   \n",
       "QDA                 0.362195      0.115624      2.979891     0.449799   \n",
       "TensorizedQDA       0.377502      0.087030      1.270504     0.166855   \n",
       "\n",
       "               mean_accuracy  train_mem_mean_mb  train_mem_std_mb  \\\n",
       "model                                                               \n",
       "QDA                 0.982407           0.018380          0.000724   \n",
       "TensorizedQDA       0.982593           0.018187          0.000661   \n",
       "\n",
       "               test_mem_mean_mb  test_mem_std_mb  \n",
       "model                                             \n",
       "QDA                    0.008446         0.001775  \n",
       "TensorizedQDA          0.012032         0.000070  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# hacemos un summary\n",
    "b.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 143
    },
    "id": "09eKXqlXhwL-",
    "outputId": "d42734a6-6fd8-4b15-da84-144d4923113f"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train_mean_ms</th>\n",
       "      <th>test_mean_ms</th>\n",
       "      <th>mean_accuracy</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>QDA</th>\n",
       "      <td>0.362195</td>\n",
       "      <td>2.979891</td>\n",
       "      <td>0.982407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TensorizedQDA</th>\n",
       "      <td>0.377502</td>\n",
       "      <td>1.270504</td>\n",
       "      <td>0.982593</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               train_mean_ms  test_mean_ms  mean_accuracy\n",
       "model                                                    \n",
       "QDA                 0.362195      2.979891       0.982407\n",
       "TensorizedQDA       0.377502      1.270504       0.982593"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# son muchos datos! nos quedamos con un par nomás\n",
    "summ = b.summary()\n",
    "\n",
    "# como es un pandas DataFrame, subseteamos columnas fácil\n",
    "summ[['train_mean_ms', 'test_mean_ms','mean_accuracy']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 163
    },
    "id": "EopB9574h8I5",
    "outputId": "c2bd86ab-3ba3-456d-c99b-d3bd131af75f"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train_mean_ms</th>\n",
       "      <th>train_std_ms</th>\n",
       "      <th>test_mean_ms</th>\n",
       "      <th>test_std_ms</th>\n",
       "      <th>mean_accuracy</th>\n",
       "      <th>train_mem_mean_mb</th>\n",
       "      <th>train_mem_std_mb</th>\n",
       "      <th>test_mem_mean_mb</th>\n",
       "      <th>test_mem_std_mb</th>\n",
       "      <th>train_speedup</th>\n",
       "      <th>test_speedup</th>\n",
       "      <th>train_mem_reduction</th>\n",
       "      <th>test_mem_reduction</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>QDA</th>\n",
       "      <td>0.362195</td>\n",
       "      <td>0.115624</td>\n",
       "      <td>2.979891</td>\n",
       "      <td>0.449799</td>\n",
       "      <td>0.982407</td>\n",
       "      <td>0.018380</td>\n",
       "      <td>0.000724</td>\n",
       "      <td>0.008446</td>\n",
       "      <td>0.001775</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TensorizedQDA</th>\n",
       "      <td>0.377502</td>\n",
       "      <td>0.087030</td>\n",
       "      <td>1.270504</td>\n",
       "      <td>0.166855</td>\n",
       "      <td>0.982593</td>\n",
       "      <td>0.018187</td>\n",
       "      <td>0.000661</td>\n",
       "      <td>0.012032</td>\n",
       "      <td>0.000070</td>\n",
       "      <td>0.959452</td>\n",
       "      <td>2.345441</td>\n",
       "      <td>1.01064</td>\n",
       "      <td>0.701904</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               train_mean_ms  train_std_ms  test_mean_ms  test_std_ms  \\\n",
       "model                                                                   \n",
       "QDA                 0.362195      0.115624      2.979891     0.449799   \n",
       "TensorizedQDA       0.377502      0.087030      1.270504     0.166855   \n",
       "\n",
       "               mean_accuracy  train_mem_mean_mb  train_mem_std_mb  \\\n",
       "model                                                               \n",
       "QDA                 0.982407           0.018380          0.000724   \n",
       "TensorizedQDA       0.982593           0.018187          0.000661   \n",
       "\n",
       "               test_mem_mean_mb  test_mem_std_mb  train_speedup  test_speedup  \\\n",
       "model                                                                           \n",
       "QDA                    0.008446         0.001775       1.000000      1.000000   \n",
       "TensorizedQDA          0.012032         0.000070       0.959452      2.345441   \n",
       "\n",
       "               train_mem_reduction  test_mem_reduction  \n",
       "model                                                   \n",
       "QDA                        1.00000            1.000000  \n",
       "TensorizedQDA              1.01064            0.701904  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# podemos setear un baseline para que fabrique columnas de comparación\n",
    "summ = b.summary(baseline='QDA')\n",
    "\n",
    "summ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 143
    },
    "id": "z0qeE1gviFLZ",
    "outputId": "26f288da-88c0-4568-d4cb-f3d60bf5045c"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train_mean_ms</th>\n",
       "      <th>test_mean_ms</th>\n",
       "      <th>mean_accuracy</th>\n",
       "      <th>train_speedup</th>\n",
       "      <th>test_speedup</th>\n",
       "      <th>train_mem_reduction</th>\n",
       "      <th>test_mem_reduction</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>QDA</th>\n",
       "      <td>0.362195</td>\n",
       "      <td>2.979891</td>\n",
       "      <td>0.982407</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TensorizedQDA</th>\n",
       "      <td>0.377502</td>\n",
       "      <td>1.270504</td>\n",
       "      <td>0.982593</td>\n",
       "      <td>0.959452</td>\n",
       "      <td>2.345441</td>\n",
       "      <td>1.01064</td>\n",
       "      <td>0.701904</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               train_mean_ms  test_mean_ms  mean_accuracy  train_speedup  \\\n",
       "model                                                                      \n",
       "QDA                 0.362195      2.979891       0.982407       1.000000   \n",
       "TensorizedQDA       0.377502      1.270504       0.982593       0.959452   \n",
       "\n",
       "               test_speedup  train_mem_reduction  test_mem_reduction  \n",
       "model                                                                 \n",
       "QDA                1.000000              1.00000            1.000000  \n",
       "TensorizedQDA      2.345441              1.01064            0.701904  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summ[[\n",
    "    'train_mean_ms', 'test_mean_ms','mean_accuracy',\n",
    "    'train_speedup', 'test_speedup',\n",
    "    'train_mem_reduction', 'test_mem_reduction'\n",
    "]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EF80Pck2RmaC"
   },
   "source": [
    "# Consigna QDA\n",
    "\n",
    "**Notación**: en general notamos\n",
    "\n",
    "* $k$ la cantidad de clases\n",
    "* $n$ la cantidad de observaciones\n",
    "* $p$ la cantidad de features/variables/predictores\n",
    "\n",
    "**Sugerencia:** combinaciones adecuadas de `transpose`, `stack`, `reshape` y, ocasionalmente, `flatten` y `diagonal` suele ser más que suficiente. Se recomienda **fuertemente* explorar la dimensionalidad de cada elemento antes de implementar las clases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Tensorización\n",
    "\n",
    "En esta sección nos vamos a ocupar de hacer que el modelo sea más rápido para generar predicciones, observando que incurre en un doble `for` dado que predice en forma individual un escalar para cada observación, para cada clase. Paralelizar ambos vía tensorización suena como una gran vía de mejora de tiempos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Diferencias entre `QDA`y `TensorizedQDA`\n",
    "\n",
    "1. ¿Sobre qué paraleliza `TensorizedQDA`? ¿Sobre las $k$ clases, las $n$ observaciones a predecir, o ambas?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`QDA` hace un doble ciclo: primero sobre las observaciones y luego sobre las clases.\n",
    "\n",
    "```python\n",
    "for observación x_i:\n",
    "    for clase j:\n",
    "        calcular distancia de x_i a media de clase j usando matriz de covarianza j\n",
    "        usar distancia para obtener verosimilitud\n",
    "```\n",
    "\n",
    "Para cada observacion $i$ y cada clase $j$:\n",
    "- Resta la media de la clase: $(x_i - \\mu_j)$\n",
    "- Aplica la fórmula de distancia de Mahalanobis: $(x_i - \\mu_j)^T \\Sigma_j^{-1} (x_i - \\mu_j)$\n",
    "- Calcula una verosimilitud por clase\n",
    "\n",
    "En cambio, `TensorizedQDA` itera sobre cada observación pero no para cada clase. Vectoriza sobre las clases usando tensores (`np.stack`), evitando el segundo loop.\n",
    "\n",
    "```python\n",
    "for observación x_i:\n",
    "    calcular las verosimilitudes para todas las clases a la vez\n",
    "```\n",
    "\n",
    "Para cada observación $i$:\n",
    "- Calcula las distancias a todas las medias de clase simultáneamente\n",
    "- Evalúa la forma cuadrática para cada clase en paralelo\n",
    "- Usa broadcasting para aplicar todas las $\\Sigma_j^{-1}$ sin necesidad de bucles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`TensorizedQDA` paraleliza **sobre las $k$ clases**.\n",
    "\n",
    "Esto se logra al apilar las medias y matrices de covarianza inversas por clase en tensores (`self.tensor_means` y `self.tensor_inv_covs`), lo que permite calcular la log-verosimilitud de una observación respecto a todas las clases simultáneamente utilizando broadcasting.\n",
    "\n",
    "La paralelización ocurre en la linea: `unbiased_x = x - self.tensor_means`. Donde $x$ es una observación de dimensión $(p, 1)$ y `self.tensor_means` es una matriz de shape $(k, p, 1)$. Esta resta se vectoriza automáticamente sobre las k clases.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Analizar los shapes de `tensor_inv_covs` y `tensor_means` y explicar paso a paso cómo es que `TensorizedQDA` llega a predecir lo mismo que `QDA`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `self.tensor_means`: Tiene shape $(k, p, 1)$, contiene las medias $\\mu_j$ para cada una de las $k$ clases, en dimensión $p$.  --> Cada clase tiene un vector columna que representa la media de cada feature.\n",
    "- `self.tensor_inv_cov`: Tiene shape $(k, p, p)$, contiene las matrices inversas de covarianza $\\Sigma_j^{-1}$ para cada clase. --> Cada clase tiene una matriz cuadrada de tamaño $(p \\times p)$ que representa la inversa de su matriz de covarianza.\n",
    "\n",
    "#### Paso a Paso:\n",
    "1) Stackear los parámetros por clase\n",
    "    - En `_fit_params(X, y)` se calcula la media (`self.means`) y la inversa de la matriz de covarianza (`self.inv_covs`) por clase.\n",
    "    - Luego se apilan en tensores:\n",
    "        - `self.tensor_means`: shape `(k, p, 1)`\n",
    "        - `self.tensor_inv_cov`: shape `(k, p, p)` \n",
    "2) Restar todas las medias al mismo tiempo: `unbiased_x = x - self.tensor_means`\n",
    "    - Se calcula la diferencia entre la observacion $x$ y cada media $\\mu_j$\n",
    "    - Usando *broadcasting*, esta operación se vectoriza automáticamente sobre las $k$ clases.\n",
    "3)  Transformar en vectores fila: `unbiased_x.transpose(0, 2, 1)`\n",
    "    - Transforma los vectores columna en vectores fila. \n",
    "    - Los prepara para que esten en formato correcto para el producto matricial de Mahalanobis.\n",
    "4) Calcular la forma cuadratica vectorizada: `inner_prod = unbiased_x.transpose(0, 2, 1) @ self.tensor_inv_cov @ unbiased_x`\n",
    "    - Obtener un escalar para cada clase con la medida de distancia para cada $j$\n",
    "5) Calcular las log-verosimilitudes de cada clase: `return 0.5 * np.log(LA.det(self.tensor_inv_cov)) - 0.5 * inner_prod.flatten()`\n",
    "    - Mahalanobis: `inner_prod.flatten()`\n",
    "    - Se calcula la log-verosimilitud completa para cada clase simultaneamente.\n",
    "\n",
    "El resultado es el mismo que en QDA porque ambos implementan la misma regla de decisión bayesiana. La diferencia esta en que `QDA`lo hace con un *for* sobre cada clase y `TensorizedQDA` lo hace en paralelo para todas las clases mediante tensores y broadcasting. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Optimización\n",
    "\n",
    "Debido a la forma cuadrática de QDA, no se puede predecir para $n$ observaciones en una sola pasada (utilizar $X \\in \\mathbb{R}^{p \\times n}$ en vez de $x \\in \\mathbb{R}^p$) sin pasar por una matriz de $n \\times n$ en donde se computan todas las interacciones entre observaciones. Se puede acceder al resultado recuperando sólo la diagonal de dicha matriz, pero resulta ineficiente en tiempo y (especialmente) en memoria. Aún así, es *posible* que el modelo funcione más rápido."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Implementar el modelo `FasterQDA` (se recomienda heredarlo de `TensorizedQDA`) de manera de eliminar el ciclo for en el método predict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FasterQDA(TensorizedQDA):\n",
    "    def _predict_log_conditional(self, X):\n",
    "        \"\"\"\n",
    "        X: shape (p, n) — cada columna es una observación\n",
    "        self.tensor_means: shape (k, p, 1)\n",
    "        self.tensor_inv_cov: shape (k, p, p)\n",
    "        \"\"\"\n",
    "        p, n = X.shape\n",
    "        k = self.tensor_means.shape[0]\n",
    "\n",
    "        # Expandimos X: (p, n) → (1, p, n)\n",
    "        X_exp = X[None, :, :]                         # (1, p, n)\n",
    "        means = self.tensor_means                     # (k, p, 1)\n",
    "        diffs = X_exp - means                         # (k, p, n)\n",
    "\n",
    "        # Multiplicamos: (k, p, n) → (k, n, p)\n",
    "        diffs_T = np.transpose(diffs, axes=(0, 2, 1))  # (k, n, p)\n",
    "\n",
    "        # inv_cov: (k, p, p)\n",
    "        # Primero: A = (k, n, p) @ (k, p, p) → (k, n, p)\n",
    "        A = np.matmul(diffs_T, self.tensor_inv_cov)    # (k, n, p)\n",
    "\n",
    "        # Segundo: M_full = A @ diffs_T.transpose(0,2,1) → (k, n, n)\n",
    "        M_full = np.matmul(A, diffs)                   # (k, n, n)\n",
    "\n",
    "        # Diagonal: (k, n)\n",
    "        dists = np.array([np.diag(M_full[j]) for j in range(k)])  # (k, n)\n",
    "\n",
    "        # Log-verosimilitud\n",
    "        log_det = np.log(np.linalg.det(self.tensor_inv_cov))      # (k,)\n",
    "        log_likelihood = 0.5 * log_det[:, None] - 0.5 * dists      # (k, n)\n",
    "        return log_likelihood\n",
    "\n",
    "    def predict(self, X):\n",
    "        log_cond = self._predict_log_conditional(X)             # (k, n)\n",
    "        posterior = log_cond + self.log_a_priori[:, None]       # (k, n)\n",
    "        return np.argmax(posterior, axis=0).reshape(1, -1)      # (1, n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Diferencias entre `TensorizedQDA` y `FasterQDA`\n",
    "\n",
    "|                       | `TensorizedQDA`                           | `FasterQDA`                                      |\n",
    "|-------------------------------|-------------------------------------------|--------------------------------------------------|\n",
    "| **Paralelización**             | Solo sobre las clases $k$             | Sobre las clases $k$ y las observaciones $n$ |\n",
    "| **Método `predict()`**         | Usa un `for` sobre observaciones (hereda de `BaseBayesianClassifier`). Llama a `_predict_one()` para cada observación | Reescribe `predict()` para vectorizar sobre todas las observaciones.  Calcula todo junto en `_predict_log_conditional(X)` |\n",
    "| **Velocidad de predicción**   | Más lenta al hacer loop sobre $n$             | Más rapida al tener la vectorización completa                    |\n",
    "\n",
    "\n",
    "`FasterQDA`:\n",
    "- Elimina el bucle `for` del método `predict()`\n",
    "- Vectoriza el cálculo de log-verosimilitud para todas las observaciones de una sola vez\n",
    "- Aumenta la eficiencia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benching params:\n",
      "Total runs: 4100\n",
      "Warmup runs: 100\n",
      "Peak Memory usage runs: 2000\n",
      "Running time runs: 2000\n",
      "Train size rows (approx): 125\n",
      "Test size rows (approx): 53\n",
      "Test size fraction: 0.3\n"
     ]
    }
   ],
   "source": [
    "# pongo un par mas de corridas \n",
    "b = Benchmark(\n",
    "    X_full, y_full_encoded,\n",
    "    n_runs = 2000,\n",
    "    warmup = 100,\n",
    "    mem_runs = 2000,\n",
    "    test_sz = 0.3,\n",
    "    same_splits = False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "QDA (MEM): 100%|██████████████████████████████████████████████████████████████████████████████████████████████| 2000/2000 [00:19<00:00, 104.50it/s]\n",
      "QDA (TIME): 100%|█████████████████████████████████████████████████████████████████████████████████████████████| 2000/2000 [00:07<00:00, 267.81it/s]\n",
      "TensorizedQDA (MEM): 100%|████████████████████████████████████████████████████████████████████████████████████| 2000/2000 [00:07<00:00, 256.05it/s]\n",
      "TensorizedQDA (TIME): 100%|███████████████████████████████████████████████████████████████████████████████████| 2000/2000 [00:03<00:00, 527.76it/s]\n",
      "FasterQDA (MEM): 100%|████████████████████████████████████████████████████████████████████████████████████████| 2000/2000 [00:02<00:00, 752.78it/s]\n",
      "FasterQDA (TIME): 100%|██████████████████████████████████████████████████████████████████████████████████████| 2000/2000 [00:01<00:00, 1517.86it/s]\n"
     ]
    }
   ],
   "source": [
    "models = [QDA, TensorizedQDA, FasterQDA]\n",
    "for model in models:\n",
    "    b.bench(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train_mean_ms</th>\n",
       "      <th>test_mean_ms</th>\n",
       "      <th>mean_accuracy</th>\n",
       "      <th>train_mem_mean_mb</th>\n",
       "      <th>test_mem_mean_mb</th>\n",
       "      <th>train_speedup</th>\n",
       "      <th>test_speedup</th>\n",
       "      <th>train_mem_reduction</th>\n",
       "      <th>test_mem_reduction</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>QDA</th>\n",
       "      <td>0.352374</td>\n",
       "      <td>3.064899</td>\n",
       "      <td>0.984472</td>\n",
       "      <td>0.017990</td>\n",
       "      <td>0.007570</td>\n",
       "      <td>1.002036</td>\n",
       "      <td>0.411374</td>\n",
       "      <td>0.997944</td>\n",
       "      <td>1.588236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TensorizedQDA</th>\n",
       "      <td>0.353092</td>\n",
       "      <td>1.260820</td>\n",
       "      <td>0.983861</td>\n",
       "      <td>0.017953</td>\n",
       "      <td>0.012023</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FasterQDA</th>\n",
       "      <td>0.330305</td>\n",
       "      <td>0.082165</td>\n",
       "      <td>0.983722</td>\n",
       "      <td>0.017909</td>\n",
       "      <td>0.115292</td>\n",
       "      <td>1.068988</td>\n",
       "      <td>15.344959</td>\n",
       "      <td>1.002470</td>\n",
       "      <td>0.104286</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               train_mean_ms  test_mean_ms  mean_accuracy  train_mem_mean_mb  \\\n",
       "model                                                                          \n",
       "QDA                 0.352374      3.064899       0.984472           0.017990   \n",
       "TensorizedQDA       0.353092      1.260820       0.983861           0.017953   \n",
       "FasterQDA           0.330305      0.082165       0.983722           0.017909   \n",
       "\n",
       "               test_mem_mean_mb  train_speedup  test_speedup  \\\n",
       "model                                                          \n",
       "QDA                    0.007570       1.002036      0.411374   \n",
       "TensorizedQDA          0.012023       1.000000      1.000000   \n",
       "FasterQDA              0.115292       1.068988     15.344959   \n",
       "\n",
       "               train_mem_reduction  test_mem_reduction  \n",
       "model                                                   \n",
       "QDA                       0.997944            1.588236  \n",
       "TensorizedQDA             1.000000            1.000000  \n",
       "FasterQDA                 1.002470            0.104286  "
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summ = b.summary(baseline='TensorizedQDA')\n",
    "\n",
    "summ[[\n",
    "    'train_mean_ms', 'test_mean_ms','mean_accuracy', 'train_mem_mean_mb', 'test_mem_mean_mb',\n",
    "    'train_speedup', 'test_speedup',\n",
    "    'train_mem_reduction', 'test_mem_reduction'\n",
    "]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusiones:\n",
    "- `FasterQDA` mantiene o mejora la precisión con respecto a `TensorizedQDA`.\n",
    "- Su tiempo de predicción es muchísimo más rápido ($\\Delta ~16.57$ veces más rápido), gracias a la vectorización completa sobre todas las observaciones.\n",
    "- Consume más memoria en test ($\\Delta ~9.53$ veces más memoria), debido a la construcción de matrices de forma $n \\times n$ al calcular las formas cuadráticas de Mahalanobis para todas las clases y observaciones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Mostrar dónde aparece la mencionada matriz de $n \\times n$, donde $n$ es la cantidad de observaciones a predecir."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La matriz de $n \\times n$ aparece al calcular la distancia de Mahalanobis entre todas las observaciones $x_i$ y $x_k$, respecto a cada clase $j$.\n",
    "\n",
    "En el modelo `FasterQDA`, se computa la siguiente expresión:\n",
    "\n",
    "`M_full = np.matmul(A, diffs)`\n",
    "\n",
    "Que es equivalente a computar para cada clase $j$:\n",
    "\n",
    "$$\n",
    "M_j = (X - \\mu_j)^T \\Sigma_j^{-1} (X - \\mu_j)\n",
    "$$\n",
    "\n",
    "donde:\n",
    "- $X \\in \\mathbb{R}^{p \\times n}$: contiene las $n$ observaciones como columnas,\n",
    "- $\\mu_j \\in \\mathbb{R}^p$: media de la clase $j$,\n",
    "- $\\Sigma_j^{-1} \\in \\mathbb{R}^{p \\times p}$: matriz inversa de covarianza de la clase $j$,\n",
    "- $M_j \\in \\mathbb{R}^{n \\times n}$: matriz simétrica que contiene en su entrada $(i, k)$ el valor:\n",
    "\n",
    "$$\n",
    "M_j[i, k] = (x_i - \\mu_j)^T \\Sigma_j^{-1} (x_k - \\mu_j)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "5. Demostrar que\n",
    "$$\n",
    "diag(A \\cdot B) = \\sum_{cols} A \\odot B^T = np.sum(A \\odot B^T, axis=1)\n",
    "$$ \n",
    "\n",
    "es decir, que se puede \"esquivar\" la matriz de $n \\times n$ usando matrices de $n \\times p$. También se puede usar, de forma equivalente,\n",
    "\n",
    "$$\n",
    "np.sum(A^T \\odot B, axis=0).T\n",
    "$$\n",
    "\n",
    "queda a preferencia del alumno cuál usar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*falta escribirlo en términos matemáticos*\n",
    "\n",
    "el elemento (i,i) de A·B es la suma de los productos de la fila i de A por la columna i de B. para evitar construir la matriz de n*n, formamos C con C(i,j) = A(i,j)·B(i,j) entonces cada fila i de C contiene justo esos terminos. al sumar cada fila de C obtenemos directamente los valores de la diagonal de A·B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "6. Utilizar la propiedad antes demostrada para reimplementar la predicción del modelo `FasterQDA` de forma eficiente en un nuevo modelo `EfficientQDA`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EfficientQDA(QDA):\n",
    "    def _predict_log_conditional(self, X):\n",
    "        num_clases = len(self.inv_covs)\n",
    "        num_observaciones = X.shape[1]\n",
    "        log_condicional = np.zeros((num_clases, num_observaciones))\n",
    "        log_det_inv = [np.log(np.linalg.det(cov_inv)) for cov_inv in self.inv_covs]\n",
    "        for clase in range(num_clases):\n",
    "            diferencias = X - self.means[clase]\n",
    "            proyeccion = self.inv_covs[clase].dot(diferencias)\n",
    "            log_condicional[clase] = 0.5 * log_det_inv[clase] - 0.5 * (diferencias * proyeccion).sum(axis=0)\n",
    "        return log_condicional\n",
    "\n",
    "    def predict(self, X):\n",
    "        log_condicional = self._predict_log_conditional(X)\n",
    "        posterior = log_condicional + self.log_a_priori[:, None]\n",
    "        return np.argmax(posterior, axis=0).reshape(1, -1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "7. Comparar la performance de las 4 variantes de QDA implementadas hasta ahora (no Cholesky) ¿Qué se observa? A modo de opinión ¿Se condice con lo esperado?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EfficientQDA (MEM): 100%|█████████████████████████████████████████████████████████████████████████████████████| 2000/2000 [00:02<00:00, 775.14it/s]\n",
      "EfficientQDA (TIME): 100%|███████████████████████████████████████████████████████████████████████████████████| 2000/2000 [00:01<00:00, 1548.45it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train_mean_ms</th>\n",
       "      <th>test_mean_ms</th>\n",
       "      <th>mean_accuracy</th>\n",
       "      <th>train_mem_mean_mb</th>\n",
       "      <th>test_mem_mean_mb</th>\n",
       "      <th>train_speedup</th>\n",
       "      <th>test_speedup</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>QDA</th>\n",
       "      <td>0.352374</td>\n",
       "      <td>3.064899</td>\n",
       "      <td>0.984472</td>\n",
       "      <td>0.017990</td>\n",
       "      <td>0.007570</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TensorizedQDA</th>\n",
       "      <td>0.353092</td>\n",
       "      <td>1.260820</td>\n",
       "      <td>0.983861</td>\n",
       "      <td>0.017953</td>\n",
       "      <td>0.012023</td>\n",
       "      <td>0.997968</td>\n",
       "      <td>2.430877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FasterQDA</th>\n",
       "      <td>0.330305</td>\n",
       "      <td>0.082165</td>\n",
       "      <td>0.983722</td>\n",
       "      <td>0.017909</td>\n",
       "      <td>0.115292</td>\n",
       "      <td>1.066816</td>\n",
       "      <td>37.301706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EfficientQDA</th>\n",
       "      <td>0.312722</td>\n",
       "      <td>0.090980</td>\n",
       "      <td>0.983537</td>\n",
       "      <td>0.017918</td>\n",
       "      <td>0.029634</td>\n",
       "      <td>1.126797</td>\n",
       "      <td>33.687630</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               train_mean_ms  test_mean_ms  mean_accuracy  train_mem_mean_mb  \\\n",
       "model                                                                          \n",
       "QDA                 0.352374      3.064899       0.984472           0.017990   \n",
       "TensorizedQDA       0.353092      1.260820       0.983861           0.017953   \n",
       "FasterQDA           0.330305      0.082165       0.983722           0.017909   \n",
       "EfficientQDA        0.312722      0.090980       0.983537           0.017918   \n",
       "\n",
       "               test_mem_mean_mb  train_speedup  test_speedup  \n",
       "model                                                         \n",
       "QDA                    0.007570       1.000000      1.000000  \n",
       "TensorizedQDA          0.012023       0.997968      2.430877  \n",
       "FasterQDA              0.115292       1.066816     37.301706  \n",
       "EfficientQDA           0.029634       1.126797     33.687630  "
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models.append(EfficientQDA)\n",
    "b.bench(EfficientQDA)\n",
    "summ = b.summary(baseline='QDA')\n",
    "summ[['train_mean_ms','test_mean_ms','mean_accuracy','train_mem_mean_mb','test_mem_mean_mb','train_speedup','test_speedup']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train_speedup</th>\n",
       "      <th>test_speedup</th>\n",
       "      <th>train_mem_reduction</th>\n",
       "      <th>test_mem_reduction</th>\n",
       "      <th>mean_accuracy</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>QDA</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.984472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TensorizedQDA</th>\n",
       "      <td>0.997968</td>\n",
       "      <td>2.430877</td>\n",
       "      <td>1.002060</td>\n",
       "      <td>0.629629</td>\n",
       "      <td>0.983861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FasterQDA</th>\n",
       "      <td>1.066816</td>\n",
       "      <td>37.301706</td>\n",
       "      <td>1.004535</td>\n",
       "      <td>0.065661</td>\n",
       "      <td>0.983722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EfficientQDA</th>\n",
       "      <td>1.126797</td>\n",
       "      <td>33.687630</td>\n",
       "      <td>1.003982</td>\n",
       "      <td>0.255455</td>\n",
       "      <td>0.983537</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               train_speedup  test_speedup  train_mem_reduction  \\\n",
       "model                                                             \n",
       "QDA                 1.000000      1.000000             1.000000   \n",
       "TensorizedQDA       0.997968      2.430877             1.002060   \n",
       "FasterQDA           1.066816     37.301706             1.004535   \n",
       "EfficientQDA        1.126797     33.687630             1.003982   \n",
       "\n",
       "               test_mem_reduction  mean_accuracy  \n",
       "model                                             \n",
       "QDA                      1.000000       0.984472  \n",
       "TensorizedQDA            0.629629       0.983861  \n",
       "FasterQDA                0.065661       0.983722  \n",
       "EfficientQDA             0.255455       0.983537  "
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summ[[\"train_speedup\", \"test_speedup\", \"train_mem_reduction\", \"test_mem_reduction\", \"mean_accuracy\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TensorizedQDA triplica la velocidad con un ligero aumento de RAM; FasterQDA acelera ~35× pero dispara el consumo de memoria; EfficientQDA logra ese mismo ~35× sin el pico de RAM. Estos resultados confirman la estrategia esperada: tensorizar, vectorizar y aplicar la fórmula de la diagonal para un QDA rápido y eficiente en memoria."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Cholesky\n",
    "\n",
    "Hasta ahora todos los esfuerzos fueron enfocados en realizar una predicción más rápida. Los tiempos de entrenamiento (teóricos al menos) siguen siendo los mismos o hasta (minúsculamente) peores, dado que todas las mejoras siguen llamando al método `_fit_params` original de `QDA`.\n",
    "\n",
    "La descomposición/factorización de [Cholesky](https://en.wikipedia.org/wiki/Cholesky_decomposition#Statement) permite factorizar una matriz definida positiva $A = LL^T$ donde $L$ es una matriz triangular inferior. En particular, si bien se asume que $p \\ll n$, invertir la matriz de covarianzas $\\Sigma$ para cada clase impone un cuello de botella que podría alivianarse. Teniendo en cuenta que las matrices de covarianza son simétricas y salvo degeneración, definidas positivas, Cholesky como mínimo debería permitir invertir la matriz más rápido.\n",
    "\n",
    "*Nota: observar que calcular* $A^{-1}b$ *equivale a resolver el sistema* $Ax=b$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) Diferencias entre implementaciones de `QDA_Chol`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. Si una matriz $A$ tiene fact. de Cholesky $A=LL^T$, expresar $A^{-1}$ en términos de $L$. ¿Cómo podría esto ser útil en la forma cuadrática de QDA?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si A = LL^T, entonces A⁻¹ = (L⁻¹)^T L⁻¹. Esto es útil en QDA porque:\n",
    "\n",
    "1. La forma cuadrática en QDA es: (x-μ)ᵀ Σ⁻¹ (x-μ)\n",
    "2. Si Σ = LLᵀ, entonces Σ⁻¹ = (L⁻¹)ᵀ L⁻¹\n",
    "3. Podemos reescribir la forma cuadrática como:\n",
    "   (x-μ)ᵀ Σ⁻¹ (x-μ) = (x-μ)ᵀ (L⁻¹)ᵀ L⁻¹ (x-μ) = (L⁻¹(x-μ))ᵀ (L⁻¹(x-μ)) = ||L⁻¹(x-μ)||²\n",
    "\n",
    "Esto permite:\n",
    "- Evitar calcular explícitamente Σ⁻¹\n",
    "- Calcular la forma cuadrática resolviendo sistemas triangulares (más eficiente)\n",
    "- Mejor estabilidad numérica"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "9. Explicar las diferencias entre `QDA_Chol1`y `QDA` y cómo `QDA_Chol1` llega, paso a paso, hasta las predicciones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Diferencias entre QDA_Chol1 y QDA:\n",
    "\n",
    "1. En lugar de almacenar Σ⁻¹ (inversa de covarianza), QDA_Chol1 almacena L⁻¹ (inversa del factor de Cholesky)\n",
    "2. El cálculo de la probabilidad logarítmica se modifica para usar la factorización\n",
    "\n",
    "Pasos de QDA_Chol1:\n",
    "1. Durante el fit:\n",
    "   - Calcula la descomposición de Cholesky: Σ = LLᵀ\n",
    "   - Almacena L⁻¹ (inversa de L) para cada clase\n",
    "   - Calcula y almacena las medias μ para cada clase\n",
    "\n",
    "2. Durante la predicción (para cada punto x y clase):\n",
    "   - Calcula x centrado: x_centered = x - μ\n",
    "   - Aplica L⁻¹: y = L⁻¹ @ x_centered\n",
    "   - Calcula log_prob = log(det(Σ⁻¹)) - yᵀy\n",
    "     Donde:\n",
    "     - log(det(Σ⁻¹)) = 2*sum(log(diag(L⁻¹))) (por propiedades de Cholesky)\n",
    "     - yᵀy es la norma al cuadrado de y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "10. ¿Cuáles son las diferencias entre `QDA_Chol1`, `QDA_Chol2` y `QDA_Chol3`?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Las tres implementaciones usan Cholesky pero difieren en:\n",
    "\n",
    "1. QDA_Chol1:\n",
    "   - Calcula L⁻¹ explícitamente usando scipy.linalg.inv()\n",
    "   - Almacena L⁻¹\n",
    "   - Multiplica directamente L⁻¹ @ x_centered\n",
    "\n",
    "2. QDA_Chol2:\n",
    "   - Almacena L (no su inversa)\n",
    "   - Usa solve_triangular para resolver L y = x_centered (equivalente a y = L⁻¹x_centered pero más eficiente)\n",
    "   - log(det(Σ⁻¹)) se calcula como -2*sum(log(diag(L)))\n",
    "\n",
    "3. QDA_Chol3:\n",
    "   - Similar a QDA_Chol1 pero calcula L⁻¹ usando dtrtri de LAPACK (más eficiente para matrices triangulares)\n",
    "   - dtrtri es una rutina optimizada específicamente para invertir matrices triangulares\n",
    "\n",
    "En general:\n",
    "- QDA_Chol2 es la más eficiente (evita calcular inversas explícitas)\n",
    "- QDA_Chol3 es mejor que QDA_Chol1 pero peor que QDA_Chol2\n",
    "- QDA_Chol1 es la menos eficiente (inversión explícita)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "11. Comparar la performance de las 7 variantes de QDA implementadas hasta ahora ¿Qué se observa?¿Hay alguna de las implementaciones de `QDA_Chol` que sea claramente mejor que las demás?¿Alguna que sea peor?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "QDA_Chol1 (MEM): 100%|████████████████████████████████████████████████████████████████████████████████████████| 2000/2000 [00:12<00:00, 158.03it/s]\n",
      "QDA_Chol1 (TIME): 100%|███████████████████████████████████████████████████████████████████████████████████████| 2000/2000 [00:05<00:00, 393.30it/s]\n",
      "QDA_Chol2 (MEM): 100%|█████████████████████████████████████████████████████████████████████████████████████████| 2000/2000 [00:22<00:00, 87.81it/s]\n",
      "QDA_Chol2 (TIME): 100%|███████████████████████████████████████████████████████████████████████████████████████| 2000/2000 [00:09<00:00, 212.96it/s]\n",
      "QDA_Chol3 (MEM): 100%|████████████████████████████████████████████████████████████████████████████████████████| 2000/2000 [00:11<00:00, 180.87it/s]\n",
      "QDA_Chol3 (TIME): 100%|███████████████████████████████████████████████████████████████████████████████████████| 2000/2000 [00:04<00:00, 470.76it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train_mean_ms</th>\n",
       "      <th>test_mean_ms</th>\n",
       "      <th>mean_accuracy</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>QDA</th>\n",
       "      <td>0.352374</td>\n",
       "      <td>3.064899</td>\n",
       "      <td>0.984472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TensorizedQDA</th>\n",
       "      <td>0.353092</td>\n",
       "      <td>1.260820</td>\n",
       "      <td>0.983861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FasterQDA</th>\n",
       "      <td>0.330305</td>\n",
       "      <td>0.082165</td>\n",
       "      <td>0.983722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EfficientQDA</th>\n",
       "      <td>0.312722</td>\n",
       "      <td>0.090980</td>\n",
       "      <td>0.983537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>QDA_Chol1</th>\n",
       "      <td>0.404313</td>\n",
       "      <td>1.847268</td>\n",
       "      <td>0.983741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>QDA_Chol2</th>\n",
       "      <td>0.321017</td>\n",
       "      <td>4.061022</td>\n",
       "      <td>0.984120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>QDA_Chol3</th>\n",
       "      <td>0.289177</td>\n",
       "      <td>1.591216</td>\n",
       "      <td>0.983676</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               train_mean_ms  test_mean_ms  mean_accuracy\n",
       "model                                                    \n",
       "QDA                 0.352374      3.064899       0.984472\n",
       "TensorizedQDA       0.353092      1.260820       0.983861\n",
       "FasterQDA           0.330305      0.082165       0.983722\n",
       "EfficientQDA        0.312722      0.090980       0.983537\n",
       "QDA_Chol1           0.404313      1.847268       0.983741\n",
       "QDA_Chol2           0.321017      4.061022       0.984120\n",
       "QDA_Chol3           0.289177      1.591216       0.983676"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chol_models = [ QDA_Chol1, QDA_Chol2, QDA_Chol3]\n",
    "models  += chol_models\n",
    "\n",
    "for model in chol_models:\n",
    "    b.bench(model)\n",
    "\n",
    "summ = b.summary(baseline='QDA')\n",
    "summ[['train_mean_ms', 'test_mean_ms', 'mean_accuracy']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train_speedup</th>\n",
       "      <th>test_speedup</th>\n",
       "      <th>train_mem_reduction</th>\n",
       "      <th>test_mem_reduction</th>\n",
       "      <th>mean_accuracy</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>QDA</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.984472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TensorizedQDA</th>\n",
       "      <td>0.997968</td>\n",
       "      <td>2.430877</td>\n",
       "      <td>1.002060</td>\n",
       "      <td>0.629629</td>\n",
       "      <td>0.983861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FasterQDA</th>\n",
       "      <td>1.066816</td>\n",
       "      <td>37.301706</td>\n",
       "      <td>1.004535</td>\n",
       "      <td>0.065661</td>\n",
       "      <td>0.983722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EfficientQDA</th>\n",
       "      <td>1.126797</td>\n",
       "      <td>33.687630</td>\n",
       "      <td>1.003982</td>\n",
       "      <td>0.255455</td>\n",
       "      <td>0.983537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>QDA_Chol1</th>\n",
       "      <td>0.871539</td>\n",
       "      <td>1.659152</td>\n",
       "      <td>1.002100</td>\n",
       "      <td>0.975604</td>\n",
       "      <td>0.983741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>QDA_Chol2</th>\n",
       "      <td>1.097682</td>\n",
       "      <td>0.754711</td>\n",
       "      <td>1.009441</td>\n",
       "      <td>0.992358</td>\n",
       "      <td>0.984120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>QDA_Chol3</th>\n",
       "      <td>1.218544</td>\n",
       "      <td>1.926136</td>\n",
       "      <td>1.008721</td>\n",
       "      <td>0.994437</td>\n",
       "      <td>0.983676</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               train_speedup  test_speedup  train_mem_reduction  \\\n",
       "model                                                             \n",
       "QDA                 1.000000      1.000000             1.000000   \n",
       "TensorizedQDA       0.997968      2.430877             1.002060   \n",
       "FasterQDA           1.066816     37.301706             1.004535   \n",
       "EfficientQDA        1.126797     33.687630             1.003982   \n",
       "QDA_Chol1           0.871539      1.659152             1.002100   \n",
       "QDA_Chol2           1.097682      0.754711             1.009441   \n",
       "QDA_Chol3           1.218544      1.926136             1.008721   \n",
       "\n",
       "               test_mem_reduction  mean_accuracy  \n",
       "model                                             \n",
       "QDA                      1.000000       0.984472  \n",
       "TensorizedQDA            0.629629       0.983861  \n",
       "FasterQDA                0.065661       0.983722  \n",
       "EfficientQDA             0.255455       0.983537  \n",
       "QDA_Chol1                0.975604       0.983741  \n",
       "QDA_Chol2                0.992358       0.984120  \n",
       "QDA_Chol3                0.994437       0.983676  "
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summ[[\"train_speedup\", \"test_speedup\", \"train_mem_reduction\", \"test_mem_reduction\", \"mean_accuracy\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¿Qué se observa en general?\n",
    "\n",
    "    Consistencia en Precisión\n",
    "\n",
    "        Todas las variantes mantienen la misma precisión que el QDA original, demostrando que las optimizaciones no comprometen la calidad predictiva.\n",
    "\n",
    "    Rendimiento en Velocidad\n",
    "\n",
    "        Predicción:\n",
    "\n",
    "            Las versiones vectorizadas (FasterQDA y EfficientQDA) destacan por su velocidad significativamente mayor, gracias a la paralelización sobre observaciones y clases.\n",
    "            TensorizedQDA también supera al QDA estándar al paralelizar solo sobre clases.\n",
    "            Entre las implementaciones Cholesky, QDA_Chol3 es la única que supera ligeramente al QDA original.\n",
    "\n",
    "        Entrenamiento:\n",
    "\n",
    "            EfficientQDA y QDA_Chol2 son los más rápidos.\n",
    "            QDA_Chol1 y QDA_Chol3 tienen un costo adicional por el cálculo de inversas.\n",
    "\n",
    "    Eficiencia en Memoria\n",
    "\n",
    "        FasterQDA consume más memoria durante la predicción debido a matrices temporales.\n",
    "        Las versiones Cholesky (QDA_Chol1/2/3) son las más eficientes en memoria."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¿Alguna es claramente mejor?\n",
    "\n",
    "Entre las QDA_Chol, en términos de velocidad QDA_Chol1 y QDA_Chol3 tienen un rendimiento similar y considerablemente superior a QDA_Chol2. En términos técnicos, QDA_Chol2 es superior debido a que evita calcular inversas explícitas usando solve_triangular, lo que mejora la estabilidad numérica (especialmente con matrices casi singulares) y reduce errores de redondeo en cálculos intermedios.\n",
    "\n",
    "¿Alguna es peor?\n",
    "\n",
    "En términos de velocidad QDA_Chol2 es considerablemente mas lenta. A los efectos prácticos resulta la peor en las condiciones de prueba.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4) Optimización\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "12. Implementar el modelo `TensorizedChol` paralelizando sobre clases/observaciones según corresponda. Se recomienda heredarlo de alguna de las implementaciones de `QDA_Chol`, aunque la elección de cuál de ellas queda a cargo del alumno según lo observado en los benchmarks de puntos anteriores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como se mencionó anteriormente, si bien la segunda implementación de cholesky (sin calcular la inversa) es mas estable numericamente, resulta la mas lenta de las tres. Nuestros benchmarks se centran en eficiencia en termino de velocidad (speedup) y memoria (mem_reduction), dado que la precisión (mean_accuracy) es similar en todas las implementaciones. Como además no hay una diferencia en la eficiencia en memoria, tomamos alguna de las más rápidas para tensorizar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TensorizedQDA_Chol1(QDA_Chol1):\n",
    "    def _fit_params(self, X, y):\n",
    "        super()._fit_params(X, y)\n",
    "\n",
    "        # Stack porque queremos vectorizar sobre las clases\n",
    "        self.tensor_L_inv = np.stack(self.L_invs)        # (k, p, p)\n",
    "        self.tensor_means = np.stack(self.means)         # (k, p, 1)\n",
    "\n",
    "    def _predict_log_conditionals(self, x):\n",
    "        # x: (p, 1)\n",
    "        unbiased_x = x - self.tensor_means               # (k, p, 1)\n",
    "        y = self.tensor_L_inv @ unbiased_x               # (k, p, 1)\n",
    "        \n",
    "        # usamos axis1 y 2 en la diagonal porque necesitamos diagonal para cada clase (matriz de pxp)\n",
    "        log_det = np.log(np.prod(np.diagonal(self.tensor_L_inv, axis1=1, axis2=2), axis=1))  # (k,)\n",
    "        quad_term = 0.5 * (y ** 2).sum(axis=1).flatten()  # (k,)\n",
    "\n",
    "        return log_det - quad_term\n",
    "\n",
    "    def _predict_one(self, x):\n",
    "        return np.argmax(self.log_a_priori + self._predict_log_conditionals(x))\n",
    "\n",
    "\n",
    "class TensorizedQDA_Chol3(QDA_Chol3):\n",
    "    def _fit_params(self, X, y):\n",
    "        super()._fit_params(X, y)\n",
    "\n",
    "        self.tensor_L_invs = np.stack(self.L_invs)    # shape (k, p, p)\n",
    "        self.tensor_means = np.stack(self.means)      # shape (k, p, 1)\n",
    "\n",
    "    def _predict_log_conditionals(self, x):\n",
    "\n",
    "        unbiased_x = x - self.tensor_means            # shape (k, p, 1)\n",
    "        #print(unbiased_x.shape)\n",
    "        y = self.tensor_L_invs @ unbiased_x           # shape (k, p, 1)\n",
    "\n",
    "        quad_term = 0.5* (y**2).sum(axis=1).flatten()           # shape (k,)\n",
    "        # usamos axis1 y 2 en la diagonal porque necesitamos diagonal para cada clase (matriz de pxp)\n",
    "        log_det = np.log(self.tensor_L_invs.diagonal(axis1=1, axis2=2)).sum(axis=1)  # shape (k,)\n",
    "\n",
    "        return log_det - 0.5 * quad_term\n",
    "\n",
    "    def _predict_one(self, x):\n",
    "        return np.argmax(self.log_a_priori + self._predict_log_conditionals(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benching params:\n",
      "Total runs: 1050\n",
      "Warmup runs: 50\n",
      "Peak Memory usage runs: 500\n",
      "Running time runs: 500\n",
      "Train size rows (approx): 125\n",
      "Test size rows (approx): 53\n",
      "Test size fraction: 0.3\n"
     ]
    }
   ],
   "source": [
    "# un benchmark auxiliar para testear los cholesky tensorizados\n",
    "b_aux = Benchmark(\n",
    "    X_full, y_full_encoded,\n",
    "    n_runs = 500,\n",
    "    warmup = 50,\n",
    "    mem_runs = 500,\n",
    "    test_sz = 0.3,\n",
    "    same_splits = False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "QDA (MEM): 100%|████████████████████████████████████████████████████████████████████████████████████████████████| 500/500 [00:04<00:00, 122.56it/s]\n",
      "QDA (TIME): 100%|███████████████████████████████████████████████████████████████████████████████████████████████| 500/500 [00:01<00:00, 312.89it/s]\n",
      "TensorizedQDA_Chol1 (MEM): 100%|████████████████████████████████████████████████████████████████████████████████| 500/500 [00:01<00:00, 264.95it/s]\n",
      "TensorizedQDA_Chol1 (TIME): 100%|███████████████████████████████████████████████████████████████████████████████| 500/500 [00:00<00:00, 594.27it/s]\n",
      "TensorizedQDA_Chol3 (MEM): 100%|████████████████████████████████████████████████████████████████████████████████| 500/500 [00:01<00:00, 274.61it/s]\n",
      "TensorizedQDA_Chol3 (TIME): 100%|███████████████████████████████████████████████████████████████████████████████| 500/500 [00:00<00:00, 649.58it/s]\n"
     ]
    }
   ],
   "source": [
    "implementations = [QDA, TensorizedQDA_Chol1, TensorizedQDA_Chol3]\n",
    "\n",
    "for model in implementations:\n",
    "    b_aux.bench(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train_speedup</th>\n",
       "      <th>test_speedup</th>\n",
       "      <th>train_mem_reduction</th>\n",
       "      <th>test_mem_reduction</th>\n",
       "      <th>mean_accuracy</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>QDA</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.984778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TensorizedQDA_Chol1</th>\n",
       "      <td>0.804154</td>\n",
       "      <td>2.510083</td>\n",
       "      <td>1.001283</td>\n",
       "      <td>0.592228</td>\n",
       "      <td>0.983037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TensorizedQDA_Chol3</th>\n",
       "      <td>0.999520</td>\n",
       "      <td>2.678317</td>\n",
       "      <td>1.012306</td>\n",
       "      <td>0.600606</td>\n",
       "      <td>0.982259</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     train_speedup  test_speedup  train_mem_reduction  \\\n",
       "model                                                                   \n",
       "QDA                       1.000000      1.000000             1.000000   \n",
       "TensorizedQDA_Chol1       0.804154      2.510083             1.001283   \n",
       "TensorizedQDA_Chol3       0.999520      2.678317             1.012306   \n",
       "\n",
       "                     test_mem_reduction  mean_accuracy  \n",
       "model                                                   \n",
       "QDA                            1.000000       0.984778  \n",
       "TensorizedQDA_Chol1            0.592228       0.983037  \n",
       "TensorizedQDA_Chol3            0.600606       0.982259  "
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "summ = b_aux.summary(baseline=\"QDA\")\n",
    "summ[[\"train_speedup\", \"test_speedup\", \"train_mem_reduction\", \"test_mem_reduction\", \"mean_accuracy\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A efectos de tomar una decision sobre en cual basar el EfficientChol, miramos los benchmarks para Chol1 y Chol3.\n",
    "No se aprecian diferencias significativas. Tomamos Chol3 solo porque presenta una media de speedup superior a Chol1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "13. Implementar el modelo `EfficientChol` combinando los insights de `EfficientQDA` y `TensorizedChol`. Si se desea, se puede implementar `FasterChol` como ayuda, pero no se contempla para el punto."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comenzamos por implementar el FastChol, eliminando el loop de `_predict_one`, llamando una sola vez al método `predict`. Esto es, vectorizamos sobre las observaciones ademas de sobre las clases. De esta forma se espera eliminar el overhead de `python` de ese loop (que es el que mas iteraciones realiza). Por lo que se espera una ganancia en tiempo muy superior a la obtenida al tensorizar solo sobre las clases\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# heredamos de chol3 porque vimos que practiamente no hay diferencia de rendimiento entre cholesky 1 y 3\n",
    "class FasterQDA_Chol3(TensorizedQDA_Chol3):\n",
    "    def _predict_log_conditional(self, X):\n",
    "\n",
    "        p, n = X.shape\n",
    "        k = self.tensor_means.shape[0]\n",
    "\n",
    "        # broadcast\n",
    "        X_exp = X[None, :, :]                          # (1, p, n)\n",
    "        means = self.tensor_means                      # (k, p, 1)\n",
    "        diffs = X_exp - means                          # (k, p, n)\n",
    "\n",
    "        y = np.matmul(self.tensor_L_invs, diffs)       # (k, p, n)\n",
    "\n",
    "        quad_term = (y ** 2).sum(axis=1)                    # (k, n)\n",
    "\n",
    "        # log(prod diag L_inv) = sum log diag\n",
    "        log_det = np.log(self.tensor_L_invs.diagonal(axis1=1, axis2=2)).sum(axis=1)  # (k,)\n",
    "\n",
    "        # Final log-likelihoods\n",
    "        log_likelihoods = log_det[:, None] - 0.5 * quad_term  # (k, n)\n",
    "        return log_likelihoods\n",
    "\n",
    "    def predict(self, X):\n",
    "\n",
    "        log_cond = self._predict_log_conditional(X)             # (k, n)\n",
    "        posterior = log_cond + self.log_a_priori[:, None]       # (k, n)\n",
    "        return np.argmax(posterior, axis=0).reshape(1, -1)      # (1, n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# heredamos de faster, tienen el mismo predict\n",
    "class EfficientChol3(FasterQDA_Chol3):\n",
    "    def _fit_params(self, X, y):\n",
    "        k = self.log_a_priori.shape[0]\n",
    "        p, n = X.shape\n",
    "        self.tensor_Linv = np.zeros((k, p, p))\n",
    "        self.tensor_means = np.zeros((k, p, 1))\n",
    "\n",
    "        for idx in range(k):\n",
    "            class_X = X[:, y.flatten() == idx]\n",
    "            cov = np.cov(class_X, bias=True)\n",
    "            L = cholesky(cov, lower=True)\n",
    "            L_inv = dtrtri(L, lower=1)[0]  # L^-1\n",
    "            self.tensor_Linv[idx] = L_inv\n",
    "            self.tensor_means[idx] = class_X.mean(axis=1, keepdims=True)\n",
    "\n",
    "    def _predict_log_conditional(self, X):\n",
    "       \n",
    "        p, n = X.shape\n",
    "        k = self.tensor_Linv.shape[0]\n",
    "\n",
    "        X_exp = X[None, :, :]                      # (1, p, n)\n",
    "        means = self.tensor_means                  # (k, p, 1)\n",
    "        diffs = X_exp - means                      # (k, p, n)\n",
    "\n",
    "        Y = np.matmul(self.tensor_Linv, diffs)     # (k, p, n)\n",
    "\n",
    "        squared_norms = np.sum(Y**2, axis=1)       # (k, n)\n",
    "        log_det = np.log(np.prod(np.abs(self.tensor_Linv.diagonal(axis1=1, axis2=2)), axis=1))  # (k,)\n",
    "\n",
    "        log_likelihood = log_det[:, None] - 0.5 * squared_norms\n",
    "        return log_likelihood\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EfficientChol3 (MEM): 100%|█████████████████████████████████████████████████████████████████████████████████████| 500/500 [00:00<00:00, 940.33it/s]\n",
      "EfficientChol3 (TIME): 100%|███████████████████████████████████████████████████████████████████████████████████| 500/500 [00:00<00:00, 1785.02it/s]\n"
     ]
    }
   ],
   "source": [
    "# agregamos los dos modelos al benchmark auxiliar,\n",
    "# solo para ver que esta funcionando\n",
    "b_aux.bench(EfficientChol3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train_speedup</th>\n",
       "      <th>test_speedup</th>\n",
       "      <th>train_mem_reduction</th>\n",
       "      <th>test_mem_reduction</th>\n",
       "      <th>mean_accuracy</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>QDA</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.984778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TensorizedQDA_Chol1</th>\n",
       "      <td>0.804154</td>\n",
       "      <td>2.510083</td>\n",
       "      <td>1.001283</td>\n",
       "      <td>0.592228</td>\n",
       "      <td>0.983037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TensorizedQDA_Chol3</th>\n",
       "      <td>0.999520</td>\n",
       "      <td>2.678317</td>\n",
       "      <td>1.012306</td>\n",
       "      <td>0.600606</td>\n",
       "      <td>0.982259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EfficientChol3</th>\n",
       "      <td>1.135122</td>\n",
       "      <td>48.118461</td>\n",
       "      <td>0.725027</td>\n",
       "      <td>0.106114</td>\n",
       "      <td>0.984519</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     train_speedup  test_speedup  train_mem_reduction  \\\n",
       "model                                                                   \n",
       "QDA                       1.000000      1.000000             1.000000   \n",
       "TensorizedQDA_Chol1       0.804154      2.510083             1.001283   \n",
       "TensorizedQDA_Chol3       0.999520      2.678317             1.012306   \n",
       "EfficientChol3            1.135122     48.118461             0.725027   \n",
       "\n",
       "                     test_mem_reduction  mean_accuracy  \n",
       "model                                                   \n",
       "QDA                            1.000000       0.984778  \n",
       "TensorizedQDA_Chol1            0.592228       0.983037  \n",
       "TensorizedQDA_Chol3            0.600606       0.982259  \n",
       "EfficientChol3                 0.106114       0.984519  "
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "summ = b_aux.summary(baseline=\"QDA\")\n",
    "summ[[\"train_speedup\", \"test_speedup\", \"train_mem_reduction\", \"test_mem_reduction\", \"mean_accuracy\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un comportamiento consistente con la tensorización para el QDA regular (sin cholesky). En este caso las mejoras en velocidad son\n",
    "aun mayores.\n",
    "Se puede ver que si bien almacenar las matrices para la doble tensorización implica un mayor uso de memoria (`test_mem_reduction=0.1`), la ganancia en tiempos de ejcución es mayor incluso a las obtenidas para el método eficiente de QDA base."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "14. Comparar la performance de las 9 variantes de QDA implementadas ¿Qué se observa? A modo de opinión ¿Se condice con lo esperado?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TensorizedQDA_Chol3 (MEM): 100%|██████████████████████████████████████████████████████████████████████████████| 2000/2000 [00:07<00:00, 282.32it/s]\n",
      "TensorizedQDA_Chol3 (TIME): 100%|█████████████████████████████████████████████████████████████████████████████| 2000/2000 [00:03<00:00, 642.14it/s]\n",
      "EfficientChol3 (MEM): 100%|███████████████████████████████████████████████████████████████████████████████████| 2000/2000 [00:02<00:00, 911.93it/s]\n",
      "EfficientChol3 (TIME): 100%|█████████████████████████████████████████████████████████████████████████████████| 2000/2000 [00:01<00:00, 1730.90it/s]\n"
     ]
    }
   ],
   "source": [
    "# Agregamos los nuevos modelos al benchmark principal\n",
    "chol_tensorized_models = [TensorizedQDA_Chol3,  EfficientChol3]\n",
    "models += chol_tensorized_models # solo para registro\n",
    "for model in chol_tensorized_models:\n",
    "    b.bench(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train_speedup</th>\n",
       "      <th>test_speedup</th>\n",
       "      <th>train_mem_reduction</th>\n",
       "      <th>test_mem_reduction</th>\n",
       "      <th>mean_accuracy</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>QDA</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.984472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TensorizedQDA</th>\n",
       "      <td>0.997968</td>\n",
       "      <td>2.430877</td>\n",
       "      <td>1.002060</td>\n",
       "      <td>0.629629</td>\n",
       "      <td>0.983861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FasterQDA</th>\n",
       "      <td>1.066816</td>\n",
       "      <td>37.301706</td>\n",
       "      <td>1.004535</td>\n",
       "      <td>0.065661</td>\n",
       "      <td>0.983722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EfficientQDA</th>\n",
       "      <td>1.126797</td>\n",
       "      <td>33.687630</td>\n",
       "      <td>1.003982</td>\n",
       "      <td>0.255455</td>\n",
       "      <td>0.983537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>QDA_Chol1</th>\n",
       "      <td>0.871539</td>\n",
       "      <td>1.659152</td>\n",
       "      <td>1.002100</td>\n",
       "      <td>0.975604</td>\n",
       "      <td>0.983741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>QDA_Chol2</th>\n",
       "      <td>1.097682</td>\n",
       "      <td>0.754711</td>\n",
       "      <td>1.009441</td>\n",
       "      <td>0.992358</td>\n",
       "      <td>0.984120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>QDA_Chol3</th>\n",
       "      <td>1.218544</td>\n",
       "      <td>1.926136</td>\n",
       "      <td>1.008721</td>\n",
       "      <td>0.994437</td>\n",
       "      <td>0.983676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TensorizedQDA_Chol3</th>\n",
       "      <td>1.110294</td>\n",
       "      <td>3.111211</td>\n",
       "      <td>1.011917</td>\n",
       "      <td>0.599993</td>\n",
       "      <td>0.982380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EfficientChol3</th>\n",
       "      <td>1.248386</td>\n",
       "      <td>54.459457</td>\n",
       "      <td>0.723115</td>\n",
       "      <td>0.106026</td>\n",
       "      <td>0.983972</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     train_speedup  test_speedup  train_mem_reduction  \\\n",
       "model                                                                   \n",
       "QDA                       1.000000      1.000000             1.000000   \n",
       "TensorizedQDA             0.997968      2.430877             1.002060   \n",
       "FasterQDA                 1.066816     37.301706             1.004535   \n",
       "EfficientQDA              1.126797     33.687630             1.003982   \n",
       "QDA_Chol1                 0.871539      1.659152             1.002100   \n",
       "QDA_Chol2                 1.097682      0.754711             1.009441   \n",
       "QDA_Chol3                 1.218544      1.926136             1.008721   \n",
       "TensorizedQDA_Chol3       1.110294      3.111211             1.011917   \n",
       "EfficientChol3            1.248386     54.459457             0.723115   \n",
       "\n",
       "                     test_mem_reduction  mean_accuracy  \n",
       "model                                                   \n",
       "QDA                            1.000000       0.984472  \n",
       "TensorizedQDA                  0.629629       0.983861  \n",
       "FasterQDA                      0.065661       0.983722  \n",
       "EfficientQDA                   0.255455       0.983537  \n",
       "QDA_Chol1                      0.975604       0.983741  \n",
       "QDA_Chol2                      0.992358       0.984120  \n",
       "QDA_Chol3                      0.994437       0.983676  \n",
       "TensorizedQDA_Chol3            0.599993       0.982380  \n",
       "EfficientChol3                 0.106026       0.983972  "
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summ = b.summary(baseline='QDA')\n",
    "\n",
    "summ[[\"train_speedup\", \"test_speedup\", \"train_mem_reduction\", \"test_mem_reduction\", \"mean_accuracy\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "En primer lugar, como se destacó con los modelos anteriores, las tensorizaciones basadas en la descomposición de Cholesky no comprometen la precisión con respecto al QDA base.\n",
    "\n",
    "En general, se observa que la principal vía para mejorar el rendimiento de los modelos es eliminar bucles explícitos en Python y reemplazarlos por operaciones vectorizadas con NumPy. En particular, los mayores saltos en eficiencia se logran al aplicar **doble tensorización**: primero sobre el conjunto de clases y luego sobre las observaciones. Esta última es especialmente relevante debido a que en la mayoría de los problemas, el número de observaciones (**n**) es mucho mayor que el número de clases (**k**), por lo que evitar un bucle sobre observaciones tiene un impacto notable.\n",
    "\n",
    "Esto se refleja en los incrementos de performance observados: pasar de `TensorizedQDA` (que sólo vectoriza sobre clases) a `FasterQDA` o `EfficientQDA` (que también tensorizan sobre observaciones) implica un **salto de velocidad de aproximadamente x30** frente al modelo `QDA` original.\n",
    "\n",
    "Finalmente, al utilizar la versión `EfficientChol3`, se obtiene un **speedup cercano a x45** con respecto al `QDA` base. Esta mejora se debe a dos factores clave:\n",
    "\n",
    "1. Se mantiene la doble tensorización (sobre clases y observaciones).\n",
    "2. Se reemplaza la inversión directa de matrices de covarianza por una descomposición de Cholesky, seguida de una inversión eficiente de la matriz triangular inferior (dtrtri).\n",
    "En conjunto, estas optimizaciones explican los beneficios sustanciales observados en términos de tiempo de ejecución, sin afectar la precisión del modelo.\n",
    "\n",
    "Por último, el costo que hay que pagar para mejorar tensorizando es un mayor uso de memoria, como se observa en la degradación de los resultados para `test_mem_reduction` de las versiones tensorizadas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "0878ca0785b74f4fa8abce52c184ce33": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_9cb5b6651bac47d0b99c9388def7d2e6",
       "IPY_MODEL_ec4d02ef09c5492ab097ef02b9d3e2fb",
       "IPY_MODEL_ed4b7065efa24e5da4f69c047647c047"
      ],
      "layout": "IPY_MODEL_08dc6e2b043f409f8f8df2b63f6bc152"
     }
    },
    "08dc6e2b043f409f8f8df2b63f6bc152": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1f6553e18f4f460ebbc4b9705cfb9c3d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "26d56661a9e4490c96ef5dd831fa904b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "2add89be4e944f4fbd91c5f1d459b5cb": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_e0a05e28520841bd95e2aec19da024bd",
       "IPY_MODEL_c406ae72dbf14856971c0ac3ad078062",
       "IPY_MODEL_ca156cc465e1415b8df2a570abfa3ffe"
      ],
      "layout": "IPY_MODEL_40416335cca64bbbad37afc46049363d"
     }
    },
    "2d6622ab061342f280f7fbcbc920bc00": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f87908723b244cd99144a4e1b7be8807",
      "placeholder": "​",
      "style": "IPY_MODEL_568fd1f1b5984675bf1716520a63b7d1",
      "value": " 100/100 [00:00&lt;00:00, 180.63it/s]"
     }
    },
    "38dcdaf521fe4043b7bc5fac762867e1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "3fad52f5ac204004afac2df2b55cbc7b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ba742120ade4495d9f75b066c43b6ffb",
      "max": 100,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_e38ba66c86764d578b97aa127e0cd9ce",
      "value": 100
     }
    },
    "40416335cca64bbbad37afc46049363d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "43ef5277fff74410bb85f09e64c37cfe": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "44cc54751afe4e618d81e92e2e64ca58": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "4531ea3385d34454865e3c6ced188122": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4965b54e63794e0b80c19c150b072eda": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "4c09e94a9e494e6daa6f59c23b8bcb4f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "568fd1f1b5984675bf1716520a63b7d1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "68a99e1ecb9741f688cb33bbfc19d5d2": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6bbe1b2d6f3846168244aca7ecf761f0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_eb6e454a891d493b88fe82442a9288e2",
       "IPY_MODEL_3fad52f5ac204004afac2df2b55cbc7b",
       "IPY_MODEL_2d6622ab061342f280f7fbcbc920bc00"
      ],
      "layout": "IPY_MODEL_e7cdc4320bff45f6acf901b8bd9bdf1a"
     }
    },
    "74859eadc1ef4090a07e851f5949f4d6": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9cb5b6651bac47d0b99c9388def7d2e6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f7bc1a202cde4351a6f98ae19393cf94",
      "placeholder": "​",
      "style": "IPY_MODEL_caea67fec94c45a097cae720b582c68d",
      "value": "QDA (TIME): 100%"
     }
    },
    "9e38484f037e4f86be6b768459404767": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f1cdc4724c6d441cbebca60f1bb11aea",
      "max": 20,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_4965b54e63794e0b80c19c150b072eda",
      "value": 20
     }
    },
    "a5957e8514974187838fc2d43d548433": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a9bc2a66f8a84516a44d8d3ad8e885e2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "ab3b1739c3bf4f3ba399e3a60d043ec8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "b2d747a3bfdd4f4797f7e4b01964aa7b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "ba742120ade4495d9f75b066c43b6ffb": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "bf6f0511eeca467c870c6a261712ca7e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c136c5327b804c74b0582ed04c8825dc": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c406ae72dbf14856971c0ac3ad078062": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_43ef5277fff74410bb85f09e64c37cfe",
      "max": 20,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_d7276daf0b654112aab411ae3f14649f",
      "value": 20
     }
    },
    "ca156cc465e1415b8df2a570abfa3ffe": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_68a99e1ecb9741f688cb33bbfc19d5d2",
      "placeholder": "​",
      "style": "IPY_MODEL_cec5bcb756694268abfc4eefad72f758",
      "value": " 20/20 [00:00&lt;00:00, 37.71it/s]"
     }
    },
    "caea67fec94c45a097cae720b582c68d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "cec5bcb756694268abfc4eefad72f758": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "d670b24d966f4de1872524684ea98840": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_4c09e94a9e494e6daa6f59c23b8bcb4f",
      "placeholder": "​",
      "style": "IPY_MODEL_ab3b1739c3bf4f3ba399e3a60d043ec8",
      "value": " 20/20 [00:00&lt;00:00, 88.29it/s]"
     }
    },
    "d7276daf0b654112aab411ae3f14649f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "e0a05e28520841bd95e2aec19da024bd": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_4531ea3385d34454865e3c6ced188122",
      "placeholder": "​",
      "style": "IPY_MODEL_a9bc2a66f8a84516a44d8d3ad8e885e2",
      "value": "QDA (MEM): 100%"
     }
    },
    "e3704aaa731c464a8d6c63c39d0b167a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_ee002c5199874f8db461bb1631f198ac",
       "IPY_MODEL_9e38484f037e4f86be6b768459404767",
       "IPY_MODEL_d670b24d966f4de1872524684ea98840"
      ],
      "layout": "IPY_MODEL_74859eadc1ef4090a07e851f5949f4d6"
     }
    },
    "e38ba66c86764d578b97aa127e0cd9ce": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "e7cdc4320bff45f6acf901b8bd9bdf1a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "eb6e454a891d493b88fe82442a9288e2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_bf6f0511eeca467c870c6a261712ca7e",
      "placeholder": "​",
      "style": "IPY_MODEL_26d56661a9e4490c96ef5dd831fa904b",
      "value": "TensorizedQDA (TIME): 100%"
     }
    },
    "ec4d02ef09c5492ab097ef02b9d3e2fb": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c136c5327b804c74b0582ed04c8825dc",
      "max": 100,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_b2d747a3bfdd4f4797f7e4b01964aa7b",
      "value": 100
     }
    },
    "ed4b7065efa24e5da4f69c047647c047": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a5957e8514974187838fc2d43d548433",
      "placeholder": "​",
      "style": "IPY_MODEL_38dcdaf521fe4043b7bc5fac762867e1",
      "value": " 100/100 [00:00&lt;00:00, 114.33it/s]"
     }
    },
    "ee002c5199874f8db461bb1631f198ac": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_1f6553e18f4f460ebbc4b9705cfb9c3d",
      "placeholder": "​",
      "style": "IPY_MODEL_44cc54751afe4e618d81e92e2e64ca58",
      "value": "TensorizedQDA (MEM): 100%"
     }
    },
    "f1cdc4724c6d441cbebca60f1bb11aea": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f7bc1a202cde4351a6f98ae19393cf94": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f87908723b244cd99144a4e1b7be8807": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
